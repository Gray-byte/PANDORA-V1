\section{Conclusion}
In this paper, we propose a novel poisoning attack against concept drift adaptation with active learning.
Our attack manipulates the uncertainty ranking of testing data by injecting poisoned samples, causing a misallocation of the labeling budget and hindering effective learning of attack targets.
To reduce poisoned sample construction costs, we design a generation method that builds on naturally occurring concept drift samples.
Our approach also improves the stability of poisoned sample generation by combining problem space perturbations with uncertainty-based feature attribution.
Extensive experiments show that CDA-AL is highly vulnerable to \pandora, especially in sensitive domains like malware detection.
We further analyze existing defenses, expose their limitations, and introduce a novel filtering method tailored to the concept drift adaptation process.