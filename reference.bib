A
% Journals

% First the Full Name is given, then the abbreviation used in the AMS Math
% Reviews, with an indication if it could not be found there.
% Note the 2nd overwrites the 1st, so swap them if you want the full name.

 %{AMS}
 @String{AMSTrans = "American Mathematical Society Translations" }
 @String{AMSTrans = "Amer. Math. Soc. Transl." }
 @String{BullAMS = "Bulletin of the American Mathematical Society" }
 @String{BullAMS = "Bull. Amer. Math. Soc." }
 @String{ProcAMS = "Proceedings of the American Mathematical Society" }
 @String{ProcAMS = "Proc. Amer. Math. Soc." }
 @String{TransAMS = "Transactions of the American Mathematical Society" }
 @String{TransAMS = "Trans. Amer. Math. Soc." }

 %ACM
 @String{CACM = "Communications of the {ACM}" }
 @String{CACM = "Commun. {ACM}" }
 @String{CompServ = "Comput. Surveys" }
 @String{JACM = "J. ACM" }
 @String{ACMMathSoft = "{ACM} Transactions on Mathematical Software" }
 @String{ACMMathSoft = "{ACM} Trans. Math. Software" }
 @String{SIGNUM = "{ACM} {SIGNUM} Newsletter" }
 @String{SIGNUM = "{ACM} {SIGNUM} Newslett." }

 @String{AmerSocio = "American Journal of Sociology" }
 @String{AmerStatAssoc = "Journal of the American Statistical Association" }
 @String{AmerStatAssoc = "J. Amer. Statist. Assoc." }
 @String{ApplMathComp = "Applied Mathematics and Computation" }
 @String{ApplMathComp = "Appl. Math. Comput." }
 @String{AmerMathMonthly = "American Mathematical Monthly" }
 @String{AmerMathMonthly = "Amer. Math. Monthly" }
 @String{BIT = "{BIT}" }
 @String{BritStatPsych = "British Journal of Mathematical and Statistical
          Psychology" }
 @String{BritStatPsych = "Brit. J. Math. Statist. Psych." }
 @String{CanMathBull = "Canadian Mathematical Bulletin" }
 @String{CanMathBull = "Canad. Math. Bull." }
 @String{CompApplMath = "Journal of Computational and Applied Mathematics" }
 @String{CompApplMath = "J. Comput. Appl. Math." }
 @String{CompPhys = "Journal of Computational Physics" }
 @String{CompPhys = "J. Comput. Phys." }
 @String{CompStruct = "Computers and Structures" }
 @String{CompStruct = "Comput. \& Structures" }
 @String{CompJour = "The Computer Journal" }
 @String{CompJour = "Comput. J." }
 @String{CompSysSci = "Journal of Computer and System Sciences" }
 @String{CompSysSci = "J. Comput. System Sci." }
 @String{Computing = "Computing" }
 @String{ContempMath = "Contemporary Mathematics" }
 @String{ContempMath = "Contemp. Math." }
 @String{Crelle = "Crelle's Journal" }
 @String{GiornaleMath = "Giornale di Mathematiche" }
 @String{GiornaleMath = "Giorn. Mat." } % didn't find in AMS MR., ibid.

 %IEEE
 @String{Computer = "{IEEE} Computer" }
 @String{IEEETransComp = "{IEEE} Transactions on Computers" }
 @String{IEEETransComp = "{IEEE} Trans. Comput." }
 @String{IEEETransAC = "{IEEE} Transactions on Automatic Control" }
 @String{IEEETransAC = "{IEEE} Trans. Automat. Control" }
 @String{IEEESpec = "{IEEE} Spectrum" } % didn't find in AMS MR
 @String{ProcIEEE = "Proceedings of the {IEEE}" }
 @String{ProcIEEE = "Proc. {IEEE}" } % didn't find in AMS MR
 @String{IEEETransAeroElec = "{IEEE} Transactions on Aerospace and Electronic
     Systems" }
 @String{IEEETransAeroElec = "{IEEE} Trans. Aerospace Electron. Systems" }

 @String{IMANumerAna = "{IMA} Journal of Numerical Analysis" }
 @String{IMANumerAna = "{IMA} J. Numer. Anal." }
 @String{InfProcLet = "Information Processing Letters" }
 @String{InfProcLet = "Inform. Process. Lett." }
 @String{InstMathApp = "Journal of the Institute of Mathematics and
     its Applications" }
 @String{InstMathApp = "J. Inst. Math. Appl." }
 @String{IntControl = "International Journal of Control" }
 @String{IntControl = "Internat. J. Control" }
 @String{IntNumerEng = "International Journal for Numerical Methods in
     Engineering" }
 @String{IntNumerEng = "Internat. J. Numer. Methods Engrg." }
 @String{IntSuper = "International Journal of Supercomputing Applications" }
 @String{IntSuper = "Internat. J. Supercomputing Applic." } % didn't find
%% in AMS MR
 @String{Kibernetika = "Kibernetika" }
 @String{JResNatBurStand = "Journal of Research of the National Bureau
     of Standards" }
 @String{JResNatBurStand = "J. Res. Nat. Bur. Standards" }
 @String{LinAlgApp = "Linear Algebra and its Applications" }
 @String{LinAlgApp = "Linear Algebra Appl." }
 @String{MathAnaAppl = "Journal of Mathematical Analysis and Applications" }
 @String{MathAnaAppl = "J. Math. Anal. Appl." }
 @String{MathAnnalen = "Mathematische Annalen" }
 @String{MathAnnalen = "Math. Ann." }
 @String{MathPhys = "Journal of Mathematical Physics" }
 @String{MathPhys = "J. Math. Phys." }
 @String{MathComp = "Mathematics of Computation" }
 @String{MathComp = "Math. Comp." }
 @String{MathScand = "Mathematica Scandinavica" }
 @String{MathScand = "Math. Scand." }
 @String{TablesAidsComp = "Mathematical Tables and Other Aids to Computation" }
 @String{TablesAidsComp = "Math. Tables Aids Comput." }
 @String{NumerMath = "Numerische Mathematik" }
 @String{NumerMath = "Numer. Math." }
 @String{PacificMath = "Pacific Journal of Mathematics" }
 @String{PacificMath = "Pacific J. Math." }
 @String{ParDistComp = "Journal of Parallel and Distributed Computing" }
 @String{ParDistComp = "J. Parallel and Distrib. Comput." } % didn't find
%% in AMS MR
 @String{ParComputing = "Parallel Computing" }
 @String{ParComputing = "Parallel Comput." }
 @String{PhilMag = "Philosophical Magazine" }
 @String{PhilMag = "Philos. Mag." }
 @String{ProcNAS = "Proceedings of the National Academy of Sciences
                    of the USA" }
 @String{ProcNAS = "Proc. Nat. Acad. Sci. U. S. A." }
 @String{Psychometrika = "Psychometrika" }
 @String{QuartMath = "Quarterly Journal of Mathematics, Oxford, Series (2)" }
 @String{QuartMath = "Quart. J. Math. Oxford Ser. (2)" }
 @String{QuartApplMath = "Quarterly of Applied Mathematics" }
 @String{QuartApplMath = "Quart. Appl. Math." }
 @String{RevueInstStat = "Review of the International Statisical Institute" }
 @String{RevueInstStat = "Rev. Inst. Internat. Statist." }

 %SIAM
 @String{JSIAM = "Journal of the Society for Industrial and Applied
     Mathematics" }
 @String{JSIAM = "J. Soc. Indust. Appl. Math." }
 @String{JSIAMB = "Journal of the Society for Industrial and Applied
     Mathematics, Series B, Numerical Analysis" }
 @String{JSIAMB = "J. Soc. Indust. Appl. Math. Ser. B Numer. Anal." }
 @String{SIAMAlgMeth = "{SIAM} Journal on Algebraic and Discrete Methods" }
 @String{SIAMAlgMeth = "{SIAM} J. Algebraic Discrete Methods" }
 @String{SIAMAppMath = "{SIAM} Journal on Applied Mathematics" }
 @String{SIAMAppMath = "{SIAM} J. Appl. Math." }
 @String{SIAMComp = "{SIAM} Journal on Computing" }
 @String{SIAMComp = "{SIAM} J. Comput." }
 @String{SIAMMatrix = "{SIAM} Journal on Matrix Analysis and Applications" }
 @String{SIAMMatrix = "{SIAM} J. Matrix Anal. Appl." }
 @String{SIAMNumAnal = "{SIAM} Journal on Numerical Analysis" }
 @String{SIAMNumAnal = "{SIAM} J. Numer. Anal." }
 @String{SIAMReview = "{SIAM} Review" }
 @String{SIAMReview = "{SIAM} Rev." }
 @String{SIAMSciStat = "{SIAM} Journal on Scientific and Statistical
     Computing" }
 @String{SIAMSciStat = "{SIAM} J. Sci. Statist. Comput." }

 @String{SoftPracExp = "Software Practice and Experience" }
 @String{SoftPracExp = "Software Prac. Experience" } % didn't find in AMS MR
 @String{StatScience = "Statistical Science" }
 @String{StatScience = "Statist. Sci." }
 @String{Techno = "Technometrics" }
 @String{USSRCompMathPhys = "{USSR} Computational Mathematics and Mathematical
     Physics" }
 @String{USSRCompMathPhys = "{U. S. S. R.} Comput. Math. and Math. Phys." }
 @String{VLSICompSys = "Journal of {VLSI} and Computer Systems" }
 @String{VLSICompSys = "J. {VLSI} Comput. Syst." }
 @String{ZAngewMathMech = "Zeitschrift fur Angewandte Mathematik und
     Mechanik" }
 @String{ZAngewMathMech = "Z. Angew. Math. Mech." }
 @String{ZAngewMathPhys = "Zeitschrift fur Angewandte Mathematik und Physik" }
 @String{ZAngewMathPhys = "Z. Angew. Math. Phys." }

% Publishers % ================================================= |

 @String{Academic = "Academic Press" }
 @String{ACMPress = "{ACM} Press" }
 @String{AdamHilger = "Adam Hilger" }
 @String{AddisonWesley = "Addison-Wesley" }
 @String{AllynBacon = "Allyn and Bacon" }
 @String{AMS = "American Mathematical Society" }
 @String{Birkhauser = "Birkha{\"u}ser" }
 @String{CambridgePress = "Cambridge University Press" }
 @String{Chelsea = "Chelsea" }
 @String{ClaredonPress = "Claredon Press" }
 @String{DoverPub = "Dover Publications" }
 @String{Eyolles = "Eyolles" }
 @String{HoltRinehartWinston = "Holt, Rinehart and Winston" }
 @String{Interscience = "Interscience" }
 @String{JohnsHopkinsPress = "The Johns Hopkins University Press" }
 @String{JohnWileySons = "John Wiley and Sons" }
 @String{Macmillan = "Macmillan" }
 @String{MathWorks = "The Math Works Inc." }
 @String{McGrawHill = "McGraw-Hill" }
 @String{NatBurStd = "National Bureau of Standards" }
 @String{NorthHolland = "North-Holland" }
 @String{OxfordPress = "Oxford University Press" }  %address Oxford or London?
 @String{PergamonPress = "Pergamon Press" }
 @String{PlenumPress = "Plenum Press" }
 @String{PrenticeHall = "Prentice-Hall" }
 @String{SIAMPub = "{SIAM} Publications" }
 @String{Springer = "Springer-Verlag" }
 @String{TexasPress = "University of Texas Press" }
 @String{VanNostrand = "Van Nostrand" }
 @String{WHFreeman = "W. H. Freeman and Co." }

%Entries

% 在bib文件中添加如下基本内容，正文中正常cite即可
@misc{2024-Android-OS-Share,
	author = {Statcounter},
	year={2024},
	title = {Operating System Market Share Worldwide},
	howpublished = {\url{https://gs.statcounter.com/os-market-share}}    
}
@misc{2024-Android-Users,
	author = {coolest-gadgets},
	year={2024},
	title = {Android Statistics By Users and Revenue},
	howpublished = {\url{https://www.coolest-gadgets.com/android-statistics}}    
}
@inproceedings{2014-NDSS-Drebin,
	title={Drebin: Effective and explainable detection of android malware in your pocket.},
	author={Arp, Daniel and Spreitzenbarth, Michael and Hubner, Malte and Gascon, Hugo and Rieck, Konrad and Siemens, CERT},
	booktitle={Ndss},
	volume={14},
	pages={23--26},
	year={2014}
}
@misc{2023-Android-Malware-Downloads,
	author = {Alanna Titterington},
	year={2023},
	title = {Google Play malware clocks up more than 600 million downloads in 2023},
	howpublished = {\url{https://get.zimperium.com/mobile-banking-heists-2023/}}    
}
@article{2017-CSUR-Android-Malware-ML-Survey,
	author = {Ye, Yanfang and Li, Tao and Adjeroh, Donald and Iyengar, S. Sitharama},
	title = {A Survey on Malware Detection Using Data Mining Techniques},
	year = {2017},
	issue_date = {May 2018},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {50},
	number = {3},
	issn = {0360-0300},
	url = {https://doi.org/10.1145/3073559},
	doi = {10.1145/3073559},
	abstract = {In the Internet age, malware (such as viruses, trojans, ransomware, and bots) has posed serious and evolving security threats to Internet users. To protect legitimate users from these threats, anti-malware software products from different companies, including Comodo, Kaspersky, Kingsoft, and Symantec, provide the major defense against malware. Unfortunately, driven by the economic benefits, the number of new malware samples has explosively increased: anti-malware vendors are now confronted with millions of potential malware samples per year. In order to keep on combating the increase in malware samples, there is an urgent need to develop intelligent methods for effective and efficient malware detection from the real and large daily sample collection. In this article, we first provide a brief overview on malware as well as the anti-malware industry, and present the industrial needs on malware detection. We then survey intelligent malware detection methods. In these methods, the process of detection is usually divided into two stages: feature extraction and classification/clustering. The performance of such intelligent malware detection approaches critically depend on the extracted features and the methods for classification/clustering. We provide a comprehensive investigation on both the feature extraction and the classification/clustering techniques. We also discuss the additional issues and the challenges of malware detection using data mining techniques and finally forecast the trends of malware development.},
	journal = {ACM Comput. Surv.},
	month = {jun},
	articleno = {41},
	numpages = {40},
	keywords = {malware detection, data mining, Survey}
}
% 相关工作-后门攻击
@ARTICLE{2021-TDSC-Malware-F1-Measure,
	author={Li, Chaoran and Chen, Xiao and Wang, Derui and Wen, Sheng and Ahmed, Muhammad Ejaz and Camtepe, Seyit and Xiang, Yang},
	journal={IEEE Transactions on Dependable and Secure Computing}, 
	title={Backdoor Attack on Machine Learning Based Android Malware Detectors}, 
	year={2022},
	volume={19},
	number={5},
	pages={3357-3370},
	keywords={Malware;Detectors;Training;Feature extraction;Labeling;Computational modeling;Training data;Malware detection;backdoor attack;machine learning;computer security;data poisoning},
	doi={10.1109/TDSC.2021.3094824}
}
@inproceedings{2024-SP-Audio-Backdoor,
	title={Flowmur: A stealthy and practical audio backdoor attack with limited knowledge},
	author={Lan, Jiahe and Wang, Jie and Yan, Baochen and Yan, Zheng and Bertino, Elisa},
	booktitle={2024 IEEE Symposium on Security and Privacy (SP)},
	pages={1646--1664},
	year={2024},
	organization={IEEE}
}
@inproceedings{2024-SP-Offline-RL-Backdoor,
	title={Baffle: Hiding backdoors in offline reinforcement learning datasets},
	author={Gong, Chen and Yang, Zhou and Bai, Yunpeng and He, Junda and Shi, Jieke and Li, Kecen and Sinha, Arunesh and Xu, Bowen and Hou, Xinwen and Lo, David and others},
	booktitle={2024 IEEE Symposium on Security and Privacy (SP)},
	pages={2086--2104},
	year={2024},
	organization={IEEE}
}
@inproceedings {2023-Usenix-chenyizhen,
	author = {Yizheng Chen and Zhoujie Ding and David Wagner},
	title = {Continuous Learning for Android Malware Detection},
	booktitle = {32nd USENIX Security Symposium (USENIX Security 23)},
	year = {2023},
	isbn = {978-1-939133-37-3},
	address = {Anaheim, CA},
	pages = {1127--1144},
	publisher = {USENIX Association},
	month = aug
}
@article{liu2021comprehensive,
	title={A comprehensive active learning method for multiclass imbalanced data streams with concept drift},
	author={Liu, Weike and Zhang, Hang and Ding, Zhaoyun and Liu, Qingbao and Zhu, Cheng},
	journal={Knowledge-Based Systems},
	volume={215},
	pages={106778},
	year={2021},
	publisher={Elsevier}
}
@inproceedings{krawczyk2018combining,
	title={Combining active learning with concept drift detection for data stream mining},
	author={Krawczyk, Bartosz and Pfahringer, Bernhard and Wo{\'z}niak, Micha{\l}},
	booktitle={2018 IEEE international conference on big data (big data)},
	pages={2239--2244},
	year={2018},
	organization={IEEE}
}
@misc{2024-Android-software-Amount,
	author = {BIGOH},
	year={2024},
	title = {Top Google Play Store Statistics 2024 – Exploring the key Insights.},
	howpublished = {\url{https://bigohtech.com/google-play-store-statistics/}}  
}
@misc{Virustotaluploadinterface,
	author       = {{VirusTotal}},
	year         = {2004},
	title        = {VirusTotal: Free Online Virus, Malware and URL Scanner},
	howpublished = {\url{https://www.virustotal.com/}},
	note         = {Accessed: 2025-01-08}
}
@misc{Tencentuploadinterface,
	title = {{habo}},
	year         = {2014},
	howpublished = {\url{https://habo.qq.com/}},
	note         = {Accessed: 2025-01-06}
}
@misc{virboxprotector,
	title = {{virbox}},
	year         = {2018},
	howpublished = {\url{https://shell.virbox.com/}},
	note         = {Accessed: 2025-01-06}
}
@misc{threat-book,
	title = {{threatbook}},
	year         = {2015},
	howpublished = {\url{https://s.threatbook.com/}},
	note         = {Accessed: 2025-01-06}
}
@misc{virscan,
	title = {{virscan}},
	howpublished = {\url{https://www.virscan.org/contributor}}  
}
@misc{hybrid-analysis-interface,
	title = {{hybrid}},
	howpublished = {\url{https://www.hybrid-analysis.com/}}  
}
@misc{ApkTool,
	title = {{ApkTool}},
	howpublished = {\url{https://apktool.org/}}  
}
@misc{Kaspersky-Android-Malware-Threat-Statistics,
	title = {{IT threat evolution in Q1 2024. Mobile statistics}},
	howpublished = {\url{https://securelist.com/it-threat-evolution-q1-2024-mobile-statistics/112750/}},
	year         = {2024}
	note         = {Accessed: 2024-12-15}
}

@inproceedings {2021-Usenix-CDAE,
	title={$\{$CADE$\}$: Detecting and explaining concept drift samples for security applications},
	author={Yang, Limin and Guo, Wenbo and Hao, Qingying and Ciptadi, Arridhana and Ahmadzadeh, Ali and Xing, Xinyu and Wang, Gang},
	booktitle={30th USENIX Security Symposium (USENIX Security 21)},
	pages={2327--2344},
	year={2021}
}
@INPROCEEDINGS{2022-SP-Trancending,
	author={Barbero, Federico and Pendlebury, Feargus and Pierazzi, Fabio and Cavallaro, Lorenzo},
	booktitle={2022 IEEE Symposium on Security and Privacy (SP)}, 
	title={Transcending TRANSCEND: Revisiting Malware Classification in the Presence of Concept Drift}, 
	year={2022},
	volume={},
	number={},
	pages={805-823},
	keywords={Training;Degradation;Privacy;Pipelines;Machine learning;Prediction theory;Malware;security;machine learning;malware detection},
	doi={10.1109/SP46214.2022.9833659}
}
@InProceedings{2018-RAID-Evasion-attack-work4,
	title={Generic black-box end-to-end attack against state of the art API call based malware classifiers},
	author={Rosenberg, Ishai and Shabtai, Asaf and Rokach, Lior and Elovici, Yuval},
	booktitle={Research in Attacks, Intrusions, and Defenses: 21st International Symposium, RAID 2018, Heraklion, Crete, Greece, September 10-12, 2018, Proceedings 21},
	pages={490--510},
	year={2018},
	organization={Springer}
}
@INPROCEEDINGS{2023-SP-backdoor-attack,
	title={Jigsaw puzzle: Selective backdoor attack to subvert malware classifiers},
	author={Yang, Limin and Chen, Zhi and Cortellazzi, Jacopo and Pendlebury, Feargus and Tu, Kevin and Pierazzi, Fabio and Cavallaro, Lorenzo and Wang, Gang},
	booktitle={2023 IEEE Symposium on Security and Privacy (SP)},
	pages={719--736},
	year={2023},
	organization={IEEE}
}
@inproceedings{2016-Androzoo,
	title={Androzoo: Collecting millions of android apps for the research community},
	author={Allix, Kevin and Bissyand{\'e}, Tegawend{\'e} F and Klein, Jacques and Le Traon, Yves},
	booktitle={Proceedings of the 13th international conference on mining software repositories},
	pages={468--471},
	year={2016}
}
@article{2019-ACM-Journal-Mamadroid,
	author = {Onwuzurike, Lucky and Mariconti, Enrico and Andriotis, Panagiotis and Cristofaro, Emiliano De and Ross, Gordon and Stringhini, Gianluca},
	title = {MaMaDroid: Detecting Android Malware by Building Markov Chains of Behavioral Models (Extended Version)},
	year = {2019},
	issue_date = {May 2019},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {22},
	number = {2},
	issn = {2471-2566},
	url = {https://doi.org/10.1145/3313391},
	doi = {10.1145/3313391},
	abstract = {As Android has become increasingly popular, so has malware targeting it, thus motivating the research community to propose different detection techniques. However, the constant evolution of the Android ecosystem, and of malware itself, makes it hard to design robust tools that can operate for long periods of time without the need for modifications or costly re-training. Aiming to address this issue, we set to detect malware from a behavioral point of view, modeled as the sequence of abstracted API calls. We introduce MAMADROID, a static-analysis-based system that abstracts app’s API calls to their class, package, or family, and builds a model from their sequences obtained from the call graph of an app as Markov chains. This ensures that the model is more resilient to API changes and the features set is of manageable size. We evaluate MAMADROID using a dataset of 8.5K benign and 35.5K malicious apps collected over a period of 6 years, showing that it effectively detects malware (with up to 0.99 F-measure) and keeps its detection capabilities for long periods of time (up to 0.87 F-measure 2 years after training). We also show that MAMADROID remarkably overperforms DROIDAPIMINER, a state-of-the-art detection system that relies on the frequency of (raw) API calls. Aiming to assess whether MAMADROID’s effectiveness mainly stems from the API abstraction or from the sequencing modeling, we also evaluate a variant of it that uses frequency (instead of sequences), of abstracted API calls. We find that it is not as accurate, failing to capture maliciousness when trained on malware samples that include API calls that are equally or more frequently used by benign apps.},
	journal = {ACM Trans. Priv. Secur.},
	month = {apr},
	articleno = {14},
	numpages = {34},
	keywords = {static analysis, malware detection, Android}
}
@inproceedings{2020-CCS-APIGraph,
	title={Enhancing state-of-the-art classifiers with api semantics to detect evolved android malware},
	author={Zhang, Xiaohan and Zhang, Yuan and Zhong, Ming and Ding, Daizong and Cao, Yinzhi and Zhang, Yukun and Zhang, Mi and Yang, Min},
	booktitle={Proceedings of the 2020 ACM SIGSAC conference on computer and communications security},
	pages={757--770},
	year={2020}
}
@inproceedings{2014-NDSS-drebin,
	title={Drebin: Effective and explainable detection of android malware in your pocket.},
	author={Arp, Daniel and Spreitzenbarth, Michael and Hubner, Malte and Gascon, Hugo and Rieck, Konrad and Siemens, CERT},
	booktitle={Ndss},
	volume={14},
	number={1},
	pages={23--26},
	year={2014}
}
@misc{2021-GriftHorse-Android-Trojan-Example,
	author = {Aazim Yaswant},
	year={2021},
	title = {GriftHorse Android Trojan Steals Millions from Over 10 Million Victims Globally},
	howpublished ={\url{https://www.zimperium.com/blog/grifthorse-android-trojan-steals-millions-from-over-10-million-victims-globally/}}  
}
@INPROCEEDINGS{2020-SP-Kerckhos-principle,
	author={Pierazzi, Fabio and Pendlebury, Feargus and Cortellazzi, Jacopo and Cavallaro, Lorenzo},
	booktitle={2020 IEEE Symposium on Security and Privacy (SP)}, 
	title={Intriguing Properties of Adversarial ML Attacks in the Problem Space}, 
	year={2020},
	volume={},
	number={},
	pages={1332-1349},
	keywords={Malware;Perturbation methods;Robustness;Androids;Humanoid robots;Semantics;adversarial machine learning;problem space;input space;malware;program analysis;evasion},
	doi={10.1109/SP40000.2020.00073}
}
@article{1986-ML-Incremental-Learning-From-Noisy-Data,
	title = {Incremental learning from noisy data},
	volume = {1},
	issn = {1573-0565},
	url = {https://doi.org/10.1007/BF00116895},
	doi = {10.1007/BF00116895},
	abstract = {Induction of a concept description given noisy instances is difficult and is further exacerbated when the concepts may change over time. This paper presents a solution which has been guided by psychological and mathematical results. The method is based on a distributed concept description which is composed of a set of weighted, symbolic characterizations. Two learning processes incrementally modify this description. One adjusts the characterization weights and another creates new characterizations. The latter process is described in terms of a search through the space of possibilities and is shown to require linear space with respect to the number of attribute-value pairs in the description language. The method utilizes previously acquired concept definitions in subsequent learning by adding an attribute for each learned concept to instance descriptions. A program called STAGGER fully embodies this method, and this paper reports on a number of empirical analyses of its performance. Since understanding the relationships between a new learning method and existing ones can be difficult, this paper first reviews a framework for discussing machine learning systems and then describes STAGGER in that framework.},
	number = {3},
	journal = {Machine Learning},
	author = {Schlimmer, Jeffrey C. and Granger, Richard H.},
	month = sep,
	year = {1986},
	pages = {317--354},
}
% 注意，标签之间是不能有空格的
% 相关工作-规避攻击
@article{2019-TIFS-Evasion-Attack-Repacking-for-ML-AMD,
	title={Android HIV: A study of repackaging malware for evading machine-learning detection},
	author={Chen, Xiao and Li, Chaoran and Wang, Derui and Wen, Sheng and Zhang, Jun and Nepal, Surya and Xiang, Yang and Ren, Kui},
	journal={IEEE Transactions on Information Forensics and Security},
	volume={15},
	pages={987--1001},
	year={2019},
	publisher={IEEE}
}
@inproceedings{2021-CCS-Evasion-Attack-Graph-Attack,
	title={Structural attack against graph based android malware detection},
	author={Zhao, Kaifa and Zhou, Hao and Zhu, Yulin and Zhan, Xian and Zhou, Kai and Li, Jianfeng and Yu, Le and Yuan, Wei and Luo, Xiapu},
	booktitle={Proceedings of the 2021 ACM SIGSAC conference on computer and communications security},
	pages={3218--3235},
	year={2021}
}
% 为了说明我们的攻击相比之前的攻击对于训练数据的掌控是更弱的
% 相关工作-后门攻击
@inproceedings {2021-Usenix-Poisoning-Attack-Explanation-guided-Backdoor,
	title={$\{$Explanation-Guided$\}$ backdoor poisoning attacks against malware classifiers},
	author={Severi, Giorgio and Meyer, Jim and Coull, Scott and Oprea, Alina},
	booktitle={30th USENIX security symposium (USENIX security 21)},
	pages={1487--1504},
	year={2021}
}
@inproceedings{2023-CCS-Query-Based-Evasion-Attack,
	title={Efficient query-based attack against ML-based Android malware detection under zero knowledge setting},
	author={He, Ping and Xia, Yifan and Zhang, Xuhong and Ji, Shouling},
	booktitle={Proceedings of the 2023 ACM SIGSAC Conference on Computer and Communications Security},
	pages={90--104},
	year={2023}
}
@inproceedings{2021-CIC-dataset-paper-Entroplyzer,
	title={EntropLyzer: Android malware classification and characterization using entropy analysis of dynamic characteristics},
	author={Keyes, David Sean and Li, Beiqi and Kaur, Gurdip and Lashkari, Arash Habibi and Gagnon, Francois and Massicotte, Fr{\'e}d{\'e}ric},
	booktitle={2021 Reconciling Data Analytics, Automation, Privacy, and Security: A Big Data Challenge (RDAAPS)},
	pages={1--12},
	year={2021},
	organization={IEEE}
}
@inproceedings {2023-Usenix-Poisoning-Attack-LDP,
	author = {Xiaoguang Li and Ninghui Li and Wenhai Sun and Neil Zhenqiang Gong and Hui Li},
	title = {Fine-grained Poisoning Attack to Local Differential Privacy Protocols for Mean and Variance Estimation},
	booktitle = {32nd USENIX Security Symposium (USENIX Security 23)},
	year = {2023},
	isbn = {978-1-939133-37-3},
	address = {Anaheim, CA},
	pages = {1739--1756},
	url = {https://www.usenix.org/conference/usenixsecurity23/presentation/li-xiaoguang},
	publisher = {USENIX Association},
	month = aug
}
% uncertainty策略对应的文章，这个策略没有专门的文章，所以找了综述
@article{2023-survey-uncertainty-in-deep-neural-networks,
	title={A survey of uncertainty in deep neural networks},
	author={Gawlikowski, Jakob and Tassi, Cedrique Rovile Njieutcheu and Ali, Mohsin and Lee, Jongseok and Humt, Matthias and Feng, Jianxiang and Kruspe, Anna and Triebel, Rudolph and Jung, Peter and Roscher, Ribana and others},
	journal={Artificial Intelligence Review},
	volume={56},
	number={Suppl 1},
	pages={1513--1589},
	year={2023},
	publisher={Springer}
}
@misc{2024-SP-Test-time-poisoning-attacks,
	title={Test-Time Poisoning Attacks Against Test-Time Adaptation Models}, 
	author={Tianshuo Cong and Xinlei He and Yun Shen and Yang Zhang},
	year={2023},
	eprint={2308.08505},
	archivePrefix={arXiv},
	primaryClass={cs.CR},
	url={https://arxiv.org/abs/2308.08505}, 
}
@article{2023-TIFS-Federated-Android-Malware-ResNet,
	title={Comprehensive android malware detection based on federated learning architecture},
	author={Fang, Wenbo and He, Junjiang and Li, Wenshan and Lan, Xiaolong and Chen, Yang and Li, Tao and Huang, Jiwu and Zhang, Linlin},
	journal={IEEE Transactions on Information Forensics and Security},
	volume={18},
	pages={3977--3990},
	year={2023},
	publisher={IEEE}
}
@ARTICLE{2020-TIFS-Adversarial-deep-ensemble,
	author={Li, Deqiang and Li, Qianmu},
	journal={IEEE Transactions on Information Forensics and Security}, 
	title={Adversarial Deep Ensemble: Evasion Attacks and Defenses for Malware Detection}, 
	year={2020},
	volume={15},
	number={},
	pages={3886-3900},
	keywords={Malware;Training;Robustness;Detectors;Neural networks;Computer crime;Feature extraction;Adversarial Machine Learning;Deep Neural Networks;Ensemble;Adversarial Malware Detection},
	doi={10.1109/TIFS.2020.3003571}
}
@article{2021-ISF-Robust-android-malware-detection-system-against-adversarial-attacks-using-qlearning,
	title={Robust android malware detection system against adversarial attacks using q-learning},
	author={Rathore, Hemant and Sahay, Sanjay K and Nikam, Piyush and Sewak, Mohit},
	journal={Information Systems Frontiers},
	volume={23},
	pages={867--882},
	year={2021},
	publisher={Springer}
}
% 多面体清洁标签投毒那篇文章
@InProceedings{2019-NIPS-Transferable-clean-label-poisoning-attacks-on-deep-neural-nets,
	title={Transferable clean-label poisoning attacks on deep neural nets},
	author={Zhu, Chen and Huang, W Ronny and Li, Hengduo and Taylor, Gavin and Studer, Christoph and Goldstein, Tom},
	booktitle={International conference on machine learning},
	pages={7614--7623},
	year={2019},
	organization={PMLR}
}
% 为了说明我们是黑盒模型的假设
@inproceedings {2024-Usnix-zhuangunveiling,
	author = {Yuanxin Zhuang and Chuan Shi and Mengmei Zhang and Jinghui Chen and Lingjuan Lyu and Pan Zhou and Lichao Sun},
	title = {Unveiling the Secrets without Data: Can Graph Neural Networks Be Exploited through {Data-Free} Model Extraction Attacks?},
	booktitle = {33rd USENIX Security Symposium (USENIX Security 24)},
	pages = {5251--5268},
	year = {2024}
}
% 代理模型参考文章
@inproceedings {2024-Usenix-Modelguard-Information-theoretic-defense-against-model-extraction-attacks,
	title={Modelguard: Information-theoretic defense against model extraction attacks},
	author={Tang, Minxue and Dai, Anna and DiValentin, Louis and Ding, Aolin and Hass, Amin and Gong, Neil Zhenqiang and Chen, Yiran},
	booktitle={33rd USENIX Security Symposium (Security 2024)},
	year={2024}
}
% Trancending这篇文章依据的共形理论的原始文章
@book{2005-high-cite-Algorithmic-learning-in-a-random-world,
	title={Algorithmic learning in a random world},
	author={Vovk, Vladimir and Gammerman, Alexander and Shafer, Glenn},
	volume={29},
	year={2005},
	publisher={Springer}
}
@inproceedings{2018-SP-poisoning-regression,
	title={Manipulating machine learning: Poisoning attacks and countermeasures for regression learning},
	author={Jagielski, Matthew and Oprea, Alina and Biggio, Battista and Liu, Chang and Nita-Rotaru, Cristina and Li, Bo},
	booktitle={2018 IEEE symposium on security and privacy (SP)},
	pages={19--35},
	year={2018},
	organization={IEEE}
}
% renkui老师团队的综述，用于说明端到端清洁标签攻击效果减弱
@article{2022-ACM-Computing-Survey-Threats-to-training,
	title={Threats to training: A survey of poisoning attacks and defenses on machine learning systems},
	author={Wang, Zhibo and Ma, Jingjing and Wang, Xue and Hu, Jiahui and Qin, Zhan and Ren, Kui},
	journal={ACM Computing Surveys},
	volume={55},
	number={7},
	pages={1--36},
	year={2022},
	publisher={ACM New York, NY}
}
% 唯一找到的一篇研究主动学习投毒攻击防御的文章
@INPROCEEDINGS{2021-GLOBALCOM-acctive-learning-under-malicious-mislabeling-poisoning-attacks,
	author={Lin, Jing and Luley, Ryan and Xiong, Kaiqi},
	booktitle={2021 IEEE Global Communications Conference (GLOBECOM)}, 
	title={Active Learning Under Malicious Mislabeling and Poisoning Attacks}, 
	year={2021},
	volume={},
	number={},
	pages={1-6},
	keywords={Learning systems;Training;Deep learning;Neural networks;Natural language processing;Internet;Labeling;Machine Learning;Deep Learning (DL);Secu-rity;Data Poisoning Attack;Malicious Mislabeling},
	doi={10.1109/GLOBECOM46510.2021.9685101}
}
% 数据集收集过程中用于提取APK特征的参考论文-这篇论文有静态特征提取代码
@inproceedings{2020-Didroid-Android-malware-classification-and-characterization-using-deep-image-learning,
	title={Didroid: Android malware classification and characterization using deep image learning},
	author={Rahali, Abir and Lashkari, Arash Habibi and Kaur, Gurdip and Taheri, Laya and Gagnon, Francois and Massicotte, Fr{\'e}d{\'e}ric},
	booktitle={Proceedings of the 2020 10th International Conference on Communication and Network Security},
	pages={70--82},
	year={2020}
}
% 代理模型参考文章
@InProceedings{2023-ICLR-Exploring-model-dynamics-for-accumulative-poisoning-discovery,
	title={Exploring model dynamics for accumulative poisoning discovery},
	author={Zhu, Jianing and Guo, Xiawei and Yao, Jiangchao and Du, Chao and He, Li and Yuan, Shuo and Liu, Tongliang and Wang, Liang and Han, Bo},
	booktitle={International Conference on Machine Learning},
	pages={42983--43004},
	year={2023},
	organization={PMLR}
}
% 相关工作-规避攻击
@ARTICLE{2019-Adversarial-example-attacks-toward-android-malware-detection-system,
	author={Li, Heng and Zhou, ShiYao and Yuan, Wei and Li, Jiahuan and Leung, Henry},
	journal={IEEE Systems Journal}, 
	title={Adversarial-Example Attacks Toward Android Malware Detection System}, 
	year={2020},
	volume={14},
	number={1},
	pages={653-656},
	keywords={Malware;Gallium nitride;Generators;Detectors;Feature extraction;Training;Generative adversarial networks;Adversarial example;adversarial-example attack;generative adversarial network (GAN);malware detection},
	doi={10.1109/JSYST.2019.2906120}
}
% 相关工作-后门攻击
@inproceedings{2023-LookinOut-My-Backdoor-Investigating-Backdooring-Attacks-Against-DL-driven-Malware-Detectors,
	title={Lookin'Out My Backdoor! Investigating Backdooring Attacks Against DL-driven Malware Detectors},
	author={D'Onghia, Mario and Di Cesare, Federico and Gallo, Luigi and Carminati, Michele and Polino, Mario and Zanero, Stefano},
	booktitle={Proceedings of the 16th ACM Workshop on Artificial Intelligence and Security},
	pages={209--220},
	year={2023}
}
% 相关工作-规避攻击
@ARTICLE{2023-TDSC-Evasion-attacks-guided-by-local-explanations-against-Android-malware-classification,
	author={Shu, Zhan and Yan, Guanhua},
	journal={IEEE Transactions on Dependable and Secure Computing}, 
	title={EAGLE: Evasion Attacks Guided by Local Explanations Against Android Malware Classification}, 
	year={2024},
	volume={21},
	number={4},
	pages={3165-3182},
	keywords={Malware;Feature extraction;Operating systems;Machine learning;Robustness;Detectors;Classification algorithms;Adversarial machine learning;malware;mobile security},
	doi={10.1109/TDSC.2023.3324265}
}
% 相关工作-AMD投毒攻击-联邦学习
@inproceedings{2024-ACM-Assessing-the-Effect-of-Model-Poisoning-Attacks-on-Federated-Learning-in-Android-Malware-Detection,
	author = {Nawshin, Faria and Arnal, Romain and Unal, Devrim and Suganthan, Ponnuthurai and Touseau, Lionel},
	title = {Assessing the Effect of Model Poisoning Attacks on Federated Learning in Android Malware Detection},
	year = {2024},
	isbn = {9798400716928},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3660853.3660887},
	doi = {10.1145/3660853.3660887},
	abstract = {Android devices are central to our daily lives, which leads to an increase in mobile security threats. Attackers try to exploit vulnerabilities and steal personal information from the installed applications on these devices. Because of their widespread usage, these devices are the prime targets of cyber attacks. To get rid of this, Android malware detection has become increasingly significant. Federated learning, which is a decentralized machine learning approach, has been utilized to improve the privacy of sensitive user data. However, the integration of federated learning also introduces a vulnerability to model poisoning attacks, where adversaries deliberately bias the learning process of the model to impair the performance metrics. This paper presents a comprehensive assessment of the effect of model poisoning attacks on federated learning systems deployed for Android malware detection. We also explain an exhaustive feature selection methodology that employs both static and dynamic features of Android applications and created a novel dataset. We focus on incorporating recent malware samples while creating the dataset to make the model robust and adaptable to new malware. Furthermore, we quantify the degradation in model accuracy and reliability following a model poisoning attack scenario through a series of experiments. Additionally, we explore the defense mechanisms to mitigate the model poisoning attacks based on recent studies.},
	booktitle = {Proceedings of the Cognitive Models and Artificial Intelligence Conference},
	pages = {147–154},
	numpages = {8},
	keywords = {Android, Benign, Federated Learning, Malware, Model Poisoning Attacks, Neural Network},
	location = {undefinedstanbul, Turkiye},
	series = {AICCONF '24}
}
@misc{2018-arxiv-Evasion-attack-work1,
	title={Learning to Evade Static PE Machine Learning Malware Models via Reinforcement Learning}, 
	author={Hyrum S. Anderson and Anant Kharkar and Bobby Filar and David Evans and Phil Roth},
	year={2018},
	eprint={1801.08917},
	archivePrefix={arXiv},
	primaryClass={cs.CR},
	url={https://arxiv.org/abs/1801.08917}, 
}
@inproceedings{2020-ACM-Conference-Evasion-attack-work2,
	title={Deceiving portable executable malware classifiers into targeted misclassification with practical adversarial examples},
	author={Kucuk, Yunus and Yan, Guanhua},
	booktitle={Proceedings of the tenth ACM conference on data and application security and privacy},
	pages={341--352},
	year={2020}
}
@article{1996-ML-Learning,
	title = {Learning in the {Presence} of {Concept} {Drift} and {Hidden} {Contexts}},
	volume = {23},
	issn = {1573-0565},
	url = {https://doi.org/10.1023/A:1018046501280},
	doi = {10.1023/A:1018046501280},
	abstract = {On-line learning in domains where the target concept depends on some hidden context poses serious problems. A changing context can induce changes in the target concepts, producing what is known as concept drift. We describe a family of learning algorithms that flexibly react to concept drift and can take advantage of situations where contexts reappear. The general approach underlying all these algorithms consists of (1) keeping only a window of currently trusted examples and hypotheses; (2) storing concept descriptions and re-using them when a previous context re-appears; and (3) controlling both of these functions by a heuristic that constantly monitors the system's behavior. The paper reports on experiments that test the systems' performance under various conditions such as different levels of noise and different extent and rate of concept drift.},
	number = {1},
	journal = {Machine Learning},
	author = {Widmer, Gerhard and Kubat, Miroslav},
	month = apr,
	year = {1996},
	pages = {69--101},
}
% 攻击者挑战-数据集不受控制
@inproceedings {2024-Usenix-Neural-Network-Semantic-Backdoor-Detection-and-Mitigation,
	title={Neural Network Semantic Backdoor Detection and Mitigation: A Causality-Based Approach},
	author={Sun, Bing and Sun, Jun and Koh, Wayne and Shi, Jie},
	booktitle={Proceedings of the 33rd USENIX Security Symposium. USENIX Association, San Francisco, CA, USA},
	year={2024}
}
% 攻击者挑战-恶意软件完整性
@article{2024-Evadedroid-A-practical-evasion-attack-on-machine-learning-for-black-box-android-malware-detection,
	title = {EvadeDroid: A practical evasion attack on machine learning for black-box Android malware detection},
	journal = {Computers \& Security},
	volume = {139},
	pages = {103676},
	year = {2024},
	issn = {0167-4048},
	doi = {https://doi.org/10.1016/j.cose.2023.103676},
	author = {Hamid Bostani and Veelasha Moonsamy},
	keywords = {Query-based evasion attacks, Android malware detection, Machine learning, Black-box adversarial attacks}
}
% 攻击挑战-恶意软件完整性
@inproceedings{2022-AsiaCCS-Mab-malware-A-reinforcement-learning-framework-for-blackbox-generation-of-adversarial-malware,
	title={Mab-malware: A reinforcement learning framework for blackbox generation of adversarial malware},
	author={Song, Wei and Li, Xuezixiang and Afroz, Sadia and Garg, Deepali and Kuznetsov, Dmitry and Yin, Heng},
	booktitle={Proceedings of the 2022 ACM on Asia conference on computer and communications security},
	pages={990--1003},
	year={2022}
}
% 攻击挑战-恶意软件完整性
@inproceedings{2023-Usenix-Black-box-Adversarial-Example-Attack-towards-FCG-Based-Android-Malware-Detection-under-Incomplete-Feature-Information,
	title={Black-box Adversarial Example Attack towards $\{$FCG$\}$ Based Android Malware Detection under Incomplete Feature Information},
	author={Li, Heng and Cheng, Zhang and Wu, Bang and Yuan, Liheng and Gao, Cuiying and Yuan, Wei and Luo, Xiapu},
	booktitle={32nd USENIX Security Symposium (USENIX Security 23)},
	pages={1181--1198},
	year={2023}
}
% 攻击挑战-攻击持续性-投毒分类参考
@article{2022-ACM-Computing-Survey-Poisoning-attacks-and-countermeasures-in-ML,
	author = {Tian, Zhiyi and Cui, Lei and Liang, Jie and Yu, Shui},
	title = {A Comprehensive Survey on Poisoning Attacks and Countermeasures in Machine Learning},
	year = {2022},
	issue_date = {August 2023},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {55},
	number = {8},
	issn = {0360-0300},
	journal = {ACM Comput. Surv.},
	month = {dec},
	articleno = {166},
	numpages = {35},
	keywords = {Deep learning, federated learning, poisoning attack, backdoor attack}
}
% 攻击挑战-攻击持续性
@InProceedings{2023-CVPR-Color-backdoor-A-robust-poisoning-attack-in-color-space,
	author    = {Jiang, Wenbo and Li, Hongwei and Xu, Guowen and Zhang, Tianwei},
	title     = {Color Backdoor: A Robust Poisoning Attack in Color Space},
	booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
	month     = {June},
	year      = {2023},
	pages     = {8133-8142}
}
% 攻击挑战-攻击持续性
@InProceedings{2023-ICML-Data-poisoning-attacks-against-multimodal-encoders,
	title={Data poisoning attacks against multimodal encoders},
	author={Yang, Ziqing and He, Xinlei and Li, Zheng and Backes, Michael and Humbert, Mathias and Berrang, Pascal and Zhang, Yang},
	booktitle={International Conference on Machine Learning},
	pages={39299--39313},
	year={2023},
	organization={PMLR}
}
% 投毒攻击相关工作
@misc{2020-Industrial-scale-data-poisoning-via-gradient-matching,
	title={Witches' Brew: Industrial Scale Data Poisoning via Gradient Matching}, 
	author={Jonas Geiping and Liam Fowl and W. Ronny Huang and Wojciech Czaja and Gavin Taylor and Michael Moeller and Tom Goldstein},
	year={2021},
	eprint={2009.02276},
	archivePrefix={arXiv},
	primaryClass={cs.CV},
	url={https://arxiv.org/abs/2009.02276}, 
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 以下是没用到的参考文献
@article{rizvi2019role,
	title = {The role of demographics in online learning; A decision tree based approach},
	journal = {Computers & Education},
	volume = {137},
	pages = {32-47},
	year = {2019},
	issn = {0360-1315},
	doi = {https://doi.org/10.1016/j.compedu.2019.04.001},
	url = {https://www.sciencedirect.com/science/article/pii/S0360131519300818},
	author = {Saman Rizvi and Bart Rienties and Shakeel Ahmed Khoja},
	keywords = {Distance education and telelearning, Lifelong learning, Multimedia/hypermedia systems, Teaching/learning strategies},
	abstract = {Research has shown online learners’ performance to have a strong association with their demographic characteristics, such as regional belonging, socio-economic standing, education level, age, gender, and disability status. Despite a growing number of studies exploring factors for successful online learning outcomes, most researchers have utilised one or a combination of very few learner characteristics. Moreover, a limited number of studies scrutinised the impact of individual characteristics on learning outcomes as learners progress in a course. The current research aims to explore the dynamic impact of demographic characteristics on academic outcomes in the online learning environment. We investigated and compared the dynamic influence of six demographic characteristics on online learning outcomes using a sample of 8581 UK based learners across four Open University online courses from four different disciplines. We found region, neighborhood poverty level, and prior education respectively, to be strong predictors of overall learning outcomes. However, at a fine-grain level, such influence varied temporally as the course progressed, as well as between different courses. To conclude with, we discussed the implications for institutional support on adopting a tailored approach towards a more personalised student support system.}
}
@misc{Android-Users,
	author = {Securelist},
	year={2022},
	title = {Antivirus for Android},
	howpublished = {\url{https://www.kaspersky.com/android-antivirus}}    
}
@Book{arpachiDusseau18:osbook,
	author =       {Arpaci-Dusseau, Remzi H. and Arpaci-Dusseau Andrea C.},
	title =        {Operating Systems: Three Easy Pieces},
	publisher =    {Arpaci-Dusseau Books, LLC},
	year =         2015,
	edition =      {1.00},
	note =         {\url{http://pages.cs.wisc.edu/~remzi/OSTEP/}}
}
@InProceedings{waldspurger02,
	author =       {Waldspurger, Carl A.},
	title =        {Memory resource management in {VMware ESX} server},
	booktitle =    {USENIX Symposium on Operating System Design and
	Implementation (OSDI)},
	year =         2002,
	pages =        {181--194},
	note =         {\url{https://www.usenix.org/legacy/event/osdi02/tech/waldspurger/waldspurger.pdf}}
}
@article{2013-Malware-Detection-Survey,
	title={Malware and malware detection techniques: A survey},
	author={Landage, Jyoti and Wankhade, MP},
	journal={International Journal of Engineering Research},
	volume={2},
	number={12},
	pages={61--68},
	year={2013}
}
@misc{2023-Android-software-Amount,
	author = {Rohit Shewale},
	year={2023},
	title = {2.67 million apps are available in the Google Play Store as of March 2023.},
	howpublished = {\url{https://www.demandsage.com/android-statistics/}}  
}
@misc{2023-Android-software-Attacks,
	author = {G DATA CyberDefense},
	year={2023},
	title = {Attacks on smartphones every minute},
	howpublished = {\url{https://presse.gdata.de/news-g-data-mobile-security-report-attacks-on-smartphones-every-minute?id=174612&menueid=28982&l=english}}  
}
@misc{2024-Android-software-Attacks,
	author = {Kaspersky},
	year={2024},
	title = {IT threat evolution in Q1 2024. Mobile statistics},
	howpublished = {\url{https://securelist.com/it-threat-evolution-q1-2024-mobile-statistics/112750/}}  
}
@InProceedings{2013-book-droidapiminer,
	author="Aafer, Yousra
	and Du, Wenliang
	and Yin, Heng",
	editor="Zia, Tanveer
	and Zomaya, Albert
	and Varadharajan, Vijay
	and Mao, Morley",
	title="DroidAPIMiner: Mining API-Level Features for Robust Malware Detection in Android",
	booktitle="Security and Privacy in Communication Networks",
	year="2013",
	publisher="Springer International Publishing",
	address="Cham",
	pages="86--103",
	abstract="The increasing popularity of Android apps makes them the target of malware authors. To defend against this severe increase of Android malwares and help users make a better evaluation of apps at install time, several approaches have been proposed. However, most of these solutions suffer from some shortcomings; computationally expensive, not general or not robust enough. In this paper, we aim to mitigate Android malware installation through providing robust and lightweight classifiers. We have conducted a thorough analysis to extract relevant features to malware behavior captured at API level, and evaluated different classifiers using the generated feature set. Our results show that we are able to achieve an accuracy as high as 99{\%} and a false positive rate as low as 2.2{\%} using KNN classifier.",
	isbn="978-3-319-04283-1"
}
@ARTICLE{2018-TIFS-Droidcat,
	author={Cai, Haipeng and Meng, Na and Ryder, Barbara and Yao, Daphne},
	journal={IEEE Transactions on Information Forensics and Security}, 
	title={DroidCat: Effective Android Malware Detection and Categorization via App-Level Profiling}, 
	year={2019},
	volume={14},
	number={6},
	pages={1455-1470},
	keywords={Malware;Robustness;Security;Feature extraction;Stability analysis;Libraries;Static analysis;Android;security;malware;dynamic analysis;profiling;detection;categorization;stability;robustness;obfuscation},
	doi={10.1109/TIFS.2018.2879302}
}
@InProceedings{2017-ESORICS-Adversarial-Examples-AMDL,
	author="Grosse, Kathrin
	and Papernot, Nicolas
	and Manoharan, Praveen
	and Backes, Michael
	and McDaniel, Patrick",
	editor="Foley, Simon N.
	and Gollmann, Dieter
	and Snekkenes, Einar",
	title="Adversarial Examples for Malware Detection",
	booktitle="Computer Security -- ESORICS 2017",
	year="2017",
	publisher="Springer International Publishing",
	address="Cham",
	pages="62--79",
	abstract="Machine learning models are known to lack robustness against inputs crafted by an adversary. Such adversarial examples can, for instance, be derived from regular inputs by introducing minor---yet carefully selected---perturbations.",
	isbn="978-3-319-66399-9"
}
@misc{2019-arxiv-Adversarial-Robustness,
	title={On Evaluating Adversarial Robustness}, 
	author={Nicholas Carlini and Anish Athalye and Nicolas Papernot and Wieland Brendel and Jonas Rauber and Dimitris Tsipras and Ian Goodfellow and Aleksander Madry and Alexey Kurakin},
	year={2019},
	eprint={1902.06705},
	archivePrefix={arXiv},
	primaryClass={cs.LG},
	url={https://arxiv.org/abs/1902.06705}, 
}
@misc{2023-ZIMPERIUM-Global-Mobile-Threat-Report,
	author = {ZIMPERIUM},
	year={2023},
	title = {Global-Mobile-Threat-Report},
	howpublished = {\url{https://www.zimperium.com/global-mobile-threat-report/}}    
}
@inproceedings {2020-Usenix-Measuring-and-modeling-the-label-dynamics-of-online,
	title={Measuring and modeling the label dynamics of online $\{$Anti-Malware$\}$ engines},
	author={Zhu, Shuofei and Shi, Jianjun and Yang, Limin and Qin, Boqin and Zhang, Ziyi and Song, Linhai and Wang, Gang},
	booktitle={29th USENIX Security Symposium (USENIX Security 20)},
	pages={2361--2378},
	year={2020}
}
@article{2010-high-cite-The-Security-of-Machine-Learning,
	title = {The security of machine learning},
	volume = {81},
	issn = {1573-0565},
	url = {https://doi.org/10.1007/s10994-010-5188-5},
	doi = {10.1007/s10994-010-5188-5},
	abstract = {Machine learning’s ability to rapidly evolve to changing and complex situations has helped it become a fundamental tool for computer security. That adaptability is also a vulnerability: attackers can exploit machine learning systems. We present a taxonomy identifying and analyzing attacks against machine learning systems. We show how these classes influence the costs for the attacker and defender, and we give a formal structure defining their interaction. We use our framework to survey and analyze the literature of attacks against machine learning systems. We also illustrate our taxonomy by showing how it can guide attacks against SpamBayes, a popular statistical spam filter. Finally, we discuss how our taxonomy suggests new lines of defenses.},
	number = {2},
	journal = {Machine Learning},
	author = {Barreno, Marco and Nelson, Blaine and Joseph, Anthony D. and Tygar, J. D.},
	month = nov,
	year = {2010},
	pages = {121--148},
}
@INPROCEEDINGS{2008-SP-Casting-out-demons-Sanitizing-training-data-for-anomaly-sensors,
	author={Cretu, Gabriela F. and Stavrou, Angelos and Locasto, Michael E. and Stolfo, Salvatore J. and Keromytis, Angelos D.},
	booktitle={2008 IEEE Symposium on Security and Privacy (sp 2008)}, 
	title={Casting out Demons: Sanitizing Training Data for Anomaly Sensors}, 
	year={2008},
	volume={},
	number={},
	pages={81-95},
	keywords={Casting;Training data;Traffic control;Intrusion detection;Computer security;Data security;Data privacy;Computer science;Telecommunication traffic;Application software},
	doi={10.1109/SP.2008.11}
}
@misc{2021-ICLR-Deep-Partition-Aggregation-Provable-Defense-against-General-Poisoning-Attacks,
	title={Deep Partition Aggregation: Provable Defense against General Poisoning Attacks}, 
	author={Alexander Levine and Soheil Feizi},
	year={2021},
	eprint={2006.14768},
	archivePrefix={arXiv},
	primaryClass={cs.LG},
	url={https://arxiv.org/abs/2006.14768}, 
}
@InProceedings{2021-ICML-Improved-certified-defenses-against-data-poisoning-with-(deterministic)-finite-aggregation,
	title = 	 {Improved Certified Defenses against Data Poisoning with ({D}eterministic) Finite Aggregation},
	author =       {Wang, Wenxiao and Levine, Alexander J and Feizi, Soheil},
	booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
	pages = 	 {22769--22783},
	year = 	 {2022},
	editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
	volume = 	 {162},
	series = 	 {Proceedings of Machine Learning Research},
	month = 	 {17--23 Jul},
	publisher =    {PMLR},
	pdf = 	 {https://proceedings.mlr.press/v162/wang22m/wang22m.pdf},
	url = 	 {https://proceedings.mlr.press/v162/wang22m.html},
	abstract = 	 {Data poisoning attacks aim at manipulating model behaviors through distorting training data. Previously, an aggregation-based certified defense, Deep Partition Aggregation (DPA), was proposed to mitigate this threat. DPA predicts through an aggregation of base classifiers trained on disjoint subsets of data, thus restricting its sensitivity to dataset distortions. In this work, we propose an improved certified defense against general poisoning attacks, namely Finite Aggregation. In contrast to DPA, which directly splits the training set into disjoint subsets, our method first splits the training set into smaller disjoint subsets and then combines duplicates of them to build larger (but not disjoint) subsets for training base classifiers. This reduces the worst-case impacts of poison samples and thus improves certified robustness bounds. In addition, we offer an alternative view of our method, bridging the designs of deterministic and stochastic aggregation-based certified defenses. Empirically, our proposed Finite Aggregation consistently improves certificates on MNIST, CIFAR-10, and GTSRB, boosting certified fractions by up to 3.05%, 3.87% and 4.77%, respectively, while keeping the same clean accuracies as DPA’s, effectively establishing a new state of the art in (pointwise) certified robustness against data poisoning.}
}
@InProceedings{2022-ICML-On-collective-robustness-of-bagging-against-data-poisoning,
	title = 	 {On Collective Robustness of Bagging Against Data Poisoning},
	author =       {Chen, Ruoxin and Li, Zenan and Li, Jie and Yan, Junchi and Wu, Chentao},
	booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
	pages = 	 {3299--3319},
	year = 	 {2022},
	editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
	volume = 	 {162},
	series = 	 {Proceedings of Machine Learning Research},
	month = 	 {17--23 Jul},
	publisher =    {PMLR},
	pdf = 	 {https://proceedings.mlr.press/v162/chen22k/chen22k.pdf},
	url = 	 {https://proceedings.mlr.press/v162/chen22k.html},
	abstract = 	 {Bootstrap aggregating (bagging) is an effective ensemble protocol, which is believed can enhance robustness by its majority voting mechanism. Recent works further prove the sample-wise robustness certificates for certain forms of bagging (e.g. partition aggregation). Beyond these particular forms, in this paper, we propose the first collective certification for general bagging to compute the tight robustness against the global poisoning attack. Specifically, we compute the maximum number of simultaneously changed predictions via solving a binary integer linear programming (BILP) problem. Then we analyze the robustness of vanilla bagging and give the upper bound of the tolerable poison budget. Based on this analysis, we propose hash bagging to improve the robustness of vanilla bagging almost for free. This is achieved by modifying the random subsampling in vanilla bagging to a hash-based deterministic subsampling, as a way of controlling the influence scope for each poisoning sample universally. Our extensive experiments show the notable advantage in terms of applicability and robustness. Our code is available at https://github.com/Emiyalzn/ICML22-CRB.}
}
@inproceedings{2017-NIPS-Certified-defenses-for-data-poisoning-attacks,
	author = {Steinhardt, Jacob and Koh, Pang Wei W and Liang, Percy S},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
	pages = {},
	publisher = {Curran Associates, Inc.},
	title = {Certified Defenses for Data Poisoning Attacks},
	url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/9d7311ba459f9e45ed746755a32dcd11-Paper.pdf},
	volume = {30},
	year = {2017}
}

% 描述威胁生命周期相关工作的时候用到
@inproceedings {2022-Usenix-A-large-scale-temporal-measurement-of-android-malicious-apps-Persistence-migration-and-lessons-learned,
	author = {Yun Shen and Pierre-Antoine Vervier and Gianluca Stringhini},
	title = {A Large-scale Temporal Measurement of Android Malicious Apps: Persistence, Migration, and Lessons Learned},
	booktitle = {31st USENIX Security Symposium (USENIX Security 22)},
	year = {2022},
	isbn = {978-1-939133-31-1},
	address = {Boston, MA},
	pages = {1167--1184},
	url = {https://www.usenix.org/conference/usenixsecurity22/presentation/shen-yun},
	publisher = {USENIX Association},
	month = aug
}
% 相关工作部分-Transcending的前身
@inproceedings {2017-Usenix-Transcend,
	author = {Roberto Jordaney and Kumar Sharad and Santanu K. Dash and Zhi Wang and Davide Papini and Ilia Nouretdinov and Lorenzo Cavallaro},
	title = {Transcend: Detecting Concept Drift in Malware Classification Models},
	booktitle = {26th USENIX Security Symposium (USENIX Security 17)},
	year = {2017},
	isbn = {978-1-931971-40-9},
	address = {Vancouver, BC},
	pages = {625--642},
	url = {https://www.usenix.org/conference/usenixsecurity17/technical-sessions/presentation/jordaney},
	publisher = {USENIX Association},
	month = aug
}
% 2024年arxiv-任奎老师团队的概念漂移可解释性工作
@misc{2024-arxiv-Going-Proactive-and-Explanatory-Against-Malware-Concept-Drift,
	title={DREAM: Combating Concept Drift with Explanatory Detection and Adaptation in Malware Classification}, 
	author={Yiling He and Junchi Lei and Zhan Qin and Kui Ren},
	year={2024},
	eprint={2405.04095},
	archivePrefix={arXiv},
	primaryClass={cs.CR},
	url={https://arxiv.org/abs/2405.04095}, 
}
% 投毒攻击相关工作
@inproceedings{2017-Towards-poisoning-of-deep-learning-algorithms-with-back-gradient-optimization,
	author = {Mu\~{n}oz-Gonz\'{a}lez, Luis and Biggio, Battista and Demontis, Ambra and Paudice, Andrea and Wongrassamee, Vasin and Lupu, Emil C. and Roli, Fabio},
	title = {Towards Poisoning of Deep Learning Algorithms with Back-gradient Optimization},
	year = {2017},
	isbn = {9781450352024},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3128572.3140451},
	doi = {10.1145/3128572.3140451},
	abstract = {A number of online services nowadays rely upon machine learning to extract valuable information from data collected in the wild. This exposes learning algorithms to the threat of data poisoning, i.e., a coordinate attack in which a fraction of the training data is controlled by the attacker and manipulated to subvert the learning process. To date, these attacks have been devised only against a limited class of binary learning algorithms, due to the inherent complexity of the gradient-based procedure used to optimize the poisoning points (a.k.a. adversarial training examples). In this work, we first extend the definition of poisoning attacks to multiclass problems. We then propose a novel poisoning algorithm based on the idea of back-gradient optimization, i.e., to compute the gradient of interest through automatic differentiation, while also reversing the learning procedure to drastically reduce the attack complexity. Compared to current poisoning strategies, our approach is able to target a wider class of learning algorithms, trained with gradient-based procedures, including neural networks and deep learning architectures. We empirically evaluate its effectiveness on several application examples, including spam filtering, malware detection, and handwritten digit recognition. We finally show that, similarly to adversarial test examples, adversarial training examples can also be transferred across different learning algorithms.},
	booktitle = {Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security},
	pages = {27–38},
	numpages = {12},
	keywords = {training data poisoning, deep learning, adversarial machine learning, adversarial examples},
	location = {Dallas, Texas, USA},
	series = {AISec '17}
}
% 投毒攻击相关工作
@inproceedings{2020-NIPS-Metapoison,
	author = {Huang, W. Ronny and Geiping, Jonas and Fowl, Liam and Taylor, Gavin and Goldstein, Tom},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
	pages = {12080--12091},
	publisher = {Curran Associates, Inc.},
	title = {MetaPoison: Practical General-purpose Clean-label Data Poisoning},
	url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/8ce6fc704072e351679ac97d4a985574-Paper.pdf},
	volume = {33},
	year = {2020}
}
% 投毒攻击相关工作
@inproceedings{2019-NIPS-Learning-to-confuse,
	author = {Feng, Ji and Cai, Qi-Zhi and Zhou, Zhi-Hua},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
	pages = {},
	publisher = {Curran Associates, Inc.},
	title = {Learning to Confuse: Generating Training Time Adversarial Data with Auto-Encoder},
	url = {https://proceedings.neurips.cc/paper_files/paper/2019/file/1ce83e5d4135b07c0b82afffbe2b3436-Paper.pdf},
	volume = {32},
	year = {2019}
}
% 投毒攻击相关工作
@misc{2019-Prediction-poisoning-Towards-defenses-against-dnn-model-stealin-attacks,
	title={Prediction Poisoning: Towards Defenses Against DNN Model Stealing Attacks}, 
	author={Tribhuvanesh Orekondy and Bernt Schiele and Mario Fritz},
	year={2020},
	eprint={1906.10908},
	archivePrefix={arXiv},
	primaryClass={cs.LG},
	url={https://arxiv.org/abs/1906.10908}, 
}
% 投毒攻击相关工作
@article{2022-ML-Stronger-data-poisoning-attacks-break-data-sanitization-defenses,
	title = {Stronger data poisoning attacks break data sanitization defenses},
	volume = {111},
	issn = {1573-0565},
	url = {https://doi.org/10.1007/s10994-021-06119-y},
	doi = {10.1007/s10994-021-06119-y},
	abstract = {Machine learning models trained on data from the outside world can be corrupted by data poisoning attacks that inject malicious points into the models’ training sets. A common defense against these attacks is data sanitization: first filter out anomalous training points before training the model. In this paper, we develop three attacks that can bypass a broad range of common data sanitization defenses, including anomaly detectors based on nearest neighbors, training loss, and singular-value decomposition. By adding just 3\% poisoned data, our attacks successfully increase test error on the Enron spam detection dataset from 3 to 24\% and on the IMDB sentiment classification dataset from 12 to 29\%. In contrast, existing attacks which do not explicitly account for these data sanitization defenses are defeated by them. Our attacks are based on two ideas: (i) we coordinate our attacks to place poisoned points near one another, and (ii) we formulate each attack as a constrained optimization problem, with constraints designed to ensure that the poisoned points evade detection. As this optimization involves solving an expensive bilevel problem, our three attacks correspond to different ways of approximating this problem, based on influence functions; minimax duality; and the Karush–Kuhn–Tucker (KKT) conditions. Our results underscore the need to develop more robust defenses against data poisoning attacks.},
	number = {1},
	journal = {Machine Learning},
	author = {Koh, Pang Wei and Steinhardt, Jacob and Liang, Percy},
	month = jan,
	year = {2022},
	pages = {1--47},
}
% 投毒攻击相关工作
@inproceedings{2020-Influence-function-based-data-poisoning-attacks-to-t-recommender-systems,
	author = {Fang, Minghong and Gong, Neil Zhenqiang and Liu, Jia},
	title = {Influence Function based Data Poisoning Attacks to Top-N Recommender Systems},
	year = {2020},
	isbn = {9781450370233},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3366423.3380072},
	doi = {10.1145/3366423.3380072},
	abstract = {Recommender system is an essential component of web services to engage users. Popular recommender systems model user preferences and item properties using a large amount of crowdsourced user-item interaction data, e.g., rating scores; then top-N items that match the best with a user’s preference are recommended to the user. In this work, we show that an attacker can launch a data poisoning attack to a recommender system to make recommendations as the attacker desires via injecting fake users with carefully crafted user-item interaction data. Specifically, an attacker can trick a recommender system to recommend a target item to as many normal users as possible. We focus on matrix factorization based recommender systems because they have been widely deployed in industry. Given the number of fake users the attacker can inject, we formulate the crafting of rating scores for the fake users as an optimization problem. However, this optimization problem is challenging to solve as it is a non-convex integer programming problem. To address the challenge, we develop several techniques to approximately solve the optimization problem. For instance, we leverage influence function to select a subset of normal users who are influential to the recommendations and solve our formulated optimization problem based on these influential users. Our results show that our attacks are effective and outperform existing methods.},
	booktitle = {Proceedings of The Web Conference 2020},
	pages = {3019–3025},
	numpages = {7},
	keywords = {data poisoning attacks, adversarial machine learning., Adversarial recommender systems},
	location = {Taipei, Taiwan},
	series = {WWW '20}
}
@inproceedings{2021-Usenix-Poisoning-the-Semi-Supervised-learning,
	title={Poisoning the unlabeled dataset of $\{$Semi-Supervised$\}$ learning},
	author={Carlini, Nicholas},
	booktitle={30th USENIX Security Symposium (USENIX Security 21)},
	pages={1577--1592},
	year={2021}
}
@inproceedings{2023-SP-Jigsaw-puzzle,
	title={Jigsaw puzzle: Selective backdoor attack to subvert malware classifiers},
	author={Yang, Limin and Chen, Zhi and Cortellazzi, Jacopo and Pendlebury, Feargus and Tu, Kevin and Pierazzi, Fabio and Cavallaro, Lorenzo and Wang, Gang},
	booktitle={2023 IEEE Symposium on Security and Privacy (SP)},
	pages={719--736},
	year={2023},
	organization={IEEE}
}
%机器学习成功
@inproceedings{2024-AAAI-Stock-Price-Forecasting,
	title={MASTER: Market-Guided Stock Transformer for Stock Price Forecasting},
	author={Li, Tong and Liu, Zhaoyang and Shen, Yanyan and Wang, Xue and Chen, Haokun and Huang, Sen},
	booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
	volume={38},
	number={1},
	pages={162--170},
	year={2024}
}
@inproceedings{2024-AAAI-AoA-estimation,
	title={Model-driven deep neural network for enhanced AoA estimation using 5G gNB},
	author={Liu, Shengheng and Li, Xingkang and Mao, Zihuan and Liu, Peng and Huang, Yongming},
	booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
	volume={38},
	number={1},
	pages={214--221},
	year={2024}
}
@inproceedings{2024-AAAI-Designing-Biological-Sequences,
	title={Designing Biological Sequences without Prior Knowledge Using Evolutionary Reinforcement Learning},
	author={Zeng, Xi and Hao, Xiaotian and Tang, Hongyao and Tang, Zhentao and Jiao, Shaoqing and Lu, Dazhi and Peng, Jiajie},
	booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
	volume={38},
	number={1},
	pages={383--391},
	year={2024}
}
@inproceedings{2024-AAAI-Rumor-Detection,
	title={Propagation Tree Is Not Deep: Adaptive Graph Contrastive Learning Approach for Rumor Detection},
	author={Cui, Chaoqun and Jia, Caiyan},
	booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
	volume={38},
	number={1},
	pages={73--81},
	year={2024}
}
@inproceedings{2024-AAAI-Anomalous-User-Detection-on-Twitter,
	title={SeGA: Preference-Aware Self-Contrastive Learning with Prompts for Anomalous User Detection on Twitter},
	author={Chang, Ying-Ying and Wang, Wei-Yao and Peng, Wen-Chih},
	booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
	volume={38},
	number={1},
	pages={30--37},
	year={2024}
}
% 概念漂移问题
@article{2024-1Q-An-overview-CDA,
title={From concept drift to model degradation: An overview on performance-aware drift detectors},
author={Bayram, Firas and Ahmed, Bestoun S and Kassler, Andreas},
journal={Knowledge-Based Systems},
volume={245},
pages={108632},
year={2022},
publisher={Elsevier}
}
@article{fernando2024fesad,
	title={FeSAD ransomware detection framework with machine learning using adaption to concept drift},
	author={Fernando, Damien Warren and Komninos, Nikos},
	journal={Computers \& Security},
	volume={137},
	pages={103629},
	year={2024},
	publisher={Elsevier}
}
@article{2024-1Q-survey-CDA,
title={A survey on concept drift in process mining},
author={Sato, Denise Maria Vecino and De Freitas, Sheila Cristiana and Barddal, Jean Paul and Scalabrin, Edson Emilio},
journal={ACM Computing Surveys (CSUR)},
volume={54},
number={9},
pages={1--38},
year={2021},
publisher={ACM New York, NY}
}
@article{2018-CCF-A-concept-drift-A-review,
	title={Learning under concept drift: A review},
	author={Lu, Jie and Liu, Anjin and Dong, Fan and Gu, Feng and Gama, Joao and Zhang, Guangquan},
	journal={IEEE transactions on knowledge and data engineering},
	volume={31},
	number={12},
	pages={2346--2363},
	year={2018},
	publisher={IEEE}
}
% 主动学习后门
@inproceedings{2021-Usenix-active-learning-backdoor,
	title={$\{$Double-Cross$\}$ attacks: Subverting active learning systems},
	author={Vicarte, Jose Rodrigo Sanchez and Wang, Gang and Fletcher, Christopher W},
	booktitle={30th USENIX Security Symposium (USENIX Security 21)},
	pages={1593--1610},
	year={2021}
}
@misc{2023-Mobile-Banking-Heists,
	author = {zimperium},
	year={2023},
	title = {2023 Mobile Banking Heists Report},
	howpublished = {\url{https://www.kaspersky.co.uk/blog/malware-in-google-play-2023/26904/}}    
}
@misc{2025-Baidu-Image-Recognition,
	author = {Baidu},
	year={2025},
	title = {Universal Object and Scene Recognition},
	howpublished = {\url{https://ai.baidu.com/tech/imagerecognition/general}}    
}
% 垃圾邮件数据集
@article{2010-Spam-Emali-dataset,
	title={Tracking recurring contexts using ensemble classifiers: an application to email filtering},
	author={Katakis, Ioannis and Tsoumakas, Grigorios and Vlahavas, Ioannis},
	journal={Knowledge and Information Systems},
	volume={22},
	pages={371--391},
	year={2010},
	publisher={Springer}
}
% PE恶意软件数据集
@inproceedings{2021-PE-malware-dataset,
	title={BODMAS: An open dataset for learning based temporal analysis of PE malware},
	author={Yang, Limin and Ciptadi, Arridhana and Laziuk, Ihar and Ahmadzadeh, Ali and Wang, Gang},
	booktitle={2021 IEEE Security and Privacy Workshops (SPW)},
	pages={78--84},
	year={2021},
	organization={IEEE}
}
% MINIST数据集
@article{2017-MINIST-dataset,
	title={Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms},
	author={Xiao, Han and Rasul, Kashif and Vollgraf, Roland},
	journal={arXiv preprint arXiv:1708.07747},
	year={2017}
}
% 引言-说明CDA是研究热点
@article{2023-Model-based-explanations-of-concept-drift-Q2,
	title={Model-based explanations of concept drift},
	author={Hinder, Fabian and Vaquet, Valerie and Brinkrolf, Johannes and Hammer, Barbara},
	journal={Neurocomputing},
	volume={555},
	pages={126640},
	year={2023},
	publisher={Elsevier}
}
% 引言-说明CDA是研究热点
@article{2023-Q1-Concept-drift-handling,
	title={Concept drift handling: A domain adaptation perspective},
	author={Karimian, Mahmood and Beigy, Hamid},
	journal={Expert Systems with Applications},
	volume={224},
	pages={119946},
	year={2023},
	publisher={Elsevier}
}
% 引言-说明CDA是研究热点
@article{2023-Detecting-group-concept-drift-from-multiple-data-streams-1qu,
	title={Detecting group concept drift from multiple data streams},
	author={Yu, Hang and Liu, Weixu and Lu, Jie and Wen, Yimin and Luo, Xiangfeng and Zhang, Guangquan},
	journal={Pattern Recognition},
	volume={134},
	pages={109113},
	year={2023},
	publisher={Elsevier}
}
% 说明-CDA都是数据持续变化的
@article{2008-Just-in-time,
	title={Just-in-time adaptive classifiers—Part II: Designing the classifier},
	author={Alippi, Cesare and Roveri, Manuel},
	journal={IEEE Transactions on Neural Networks},
	volume={19},
	number={12},
	pages={2053--2064},
	year={2008},
	publisher={IEEE}
}
@article{2017-dynamic,
	title={Dynamic extreme learning machine for data stream classification},
	author={Xu, Shuliang and Wang, Junhong},
	journal={Neurocomputing},
	volume={238},
	pages={433--449},
	year={2017},
	publisher={Elsevier}
}
@inproceedings{2007-learning,
	title={Learning from time-changing data with adaptive windowing},
	author={Bifet, Albert and Gavalda, Ricard},
	booktitle={Proceedings of the 2007 SIAM international conference on data mining},
	pages={443--448},
	year={2007},
	organization={SIAM}
}
@inproceedings{2008-bach-paired,
	title={Paired learners for concept drift},
	author={Bach, Stephen H and Maloof, Marcus A},
	booktitle={2008 Eighth IEEE International Conference on Data Mining},
	pages={23--32},
	year={2008},
	organization={IEEE}
}
@inproceedings {2024-Usenix-Poisoning-Attack-on-Contribution-Evaluation,
	author = {Zhangchen Xu and Fengqing Jiang and Luyao Niu and Jinyuan Jia and Bo Li and Radha Poovendran},
	title = {{ACE}: A Model Poisoning Attack on Contribution Evaluation Methods in Federated Learning},
	booktitle = {33rd USENIX Security Symposium (USENIX Security 24)},
	year = {2024},
	isbn = {978-1-939133-44-1},
	address = {Philadelphia, PA},
	pages = {4175--4192},
	url = {https://www.usenix.org/conference/usenixsecurity24/presentation/xu-zhangchen},
	publisher = {USENIX Association},
	month = aug
}
@inproceedings{2024-Usenix-On-the-Difficulty-of-Defending-Contrastive-Learning,
	author = {Changjiang Li and Ren Pang and Bochuan Cao and Zhaohan Xi and Jinghui Chen and Shouling Ji and Ting Wang},
	title = {On the Difficulty of Defending Contrastive Learning against Backdoor Attacks},
	booktitle = {33rd USENIX Security Symposium (USENIX Security 24)},
	year = {2024},
	isbn = {978-1-939133-44-1},
	address = {Philadelphia, PA},
	pages = {2901--2918},
	url = {https://www.usenix.org/conference/usenixsecurity24/presentation/li-changjiang},
	publisher = {USENIX Association},
	month = aug
}
@article{2024-TIFS-BGAM-Backdoor,
	title={Bagm: A backdoor attack for manipulating text-to-image generative models},
	author={Vice, Jordan and Akhtar, Naveed and Hartley, Richard and Mian, Ajmal},
	journal={IEEE Transactions on Information Forensics and Security},
	year={2024},
	publisher={IEEE}
}
@article{2024-IBA-Backdoor-Attack-on-3D-Point-Cloud,
	title={iBA: Backdoor Attack on 3D Point Cloud via Reconstructing Itself},
	author={Bian, Yuhao and Tian, Shengjing and Liu, Xiuping},
	journal={IEEE Transactions on Information Forensics and Security},
	year={2024},
	publisher={IEEE}
}
@article{2024-TIFS-Backdoor-Contrastive-Learning,
	title={Manipulating Pre-Trained Encoder for Targeted Poisoning Attacks in Contrastive Learning},
	author={Chen, Jian and Gao, Yuan and Liu, Gaoyang and Abdelmoniem, Ahmed M and Wang, Chen},
	journal={IEEE Transactions on Information Forensics and Security},
	year={2024},
	publisher={IEEE}
}
@article{2023-TIFS-Backdoor-Image-Encryption,
	title={Backdoor Attack on Deep Learning-Based Medical Image Encryption and Decryption Network},
	author={Ding, Yi and Wang, Zi and Qin, Zhen and Zhou, Erqiang and Zhu, Guobin and Qin, Zhiguang and Choo, Kim-Kwang Raymond},
	journal={IEEE Transactions on Information Forensics and Security},
	year={2023},
	publisher={IEEE}
}
%@article{2024-TIFS-Backdoor-Minimalism-is-King,
%	title={Minimalism is King! High-Frequency Energy-based Screening for Data-Efficient Backdoor Attacks},
%	author={Xun, Yuan and Jia, Xiaojun and Gu, Jindong and Liu, Xinwei and Guo, Qing and Cao, Xiaochun},
%	journal={IEEE Transactions on Information Forensics and Security},
%	year={2024},
%	publisher={IEEE}
%}
@article{2018-NIPS-Poison-frogs,
	title={Poison frogs! targeted clean-label poisoning attacks on neural networks},
	author={Shafahi, Ali and Huang, W Ronny and Najibi, Mahyar and Suciu, Octavian and Studer, Christoph and Dumitras, Tudor and Goldstein, Tom},
	journal={Advances in neural information processing systems},
	volume={31},
	year={2018}
}
@inproceedings{2018-Usenix-generalized-transferability,
	title={When does machine learning $\{$FAIL$\}$? generalized transferability for evasion and poisoning attacks},
	author={Suciu, Octavian and Marginean, Radu and Kaya, Yigitcan and Daume III, Hal and Dumitras, Tudor},
	booktitle={27th USENIX Security Symposium (USENIX Security 18)},
	pages={1299--1316},
	year={2018}
}
%@inproceedings{2013-Usenix-Pollution-attacks,
%	title={Take this personally: Pollution attacks on personalized services},
%	author={Xing, Xingyu and Meng, Wei and Doozan, Dan and Snoeren, Alex C and Feamster, Nick and Lee, Wenke},
%	booktitle={22nd USENIX Security Symposium (USENIX Security 13)},
%	pages={671--686},
%	year={2013}
%}
%@article{2023-TIFS-person-re-identification-backdoor,
%	title={Invisible backdoor attack with dynamic triggers against person re-identification},
%	author={Sun, Wenli and Jiang, Xinyang and Dou, Shuguang and Li, Dongsheng and Miao, Duoqian and Deng, Cheng and Zhao, Cairong},
%	journal={IEEE Transactions on Information Forensics and Security},
%	year={2023},
%	publisher={IEEE}
%}
@inproceedings{2024-CCS-Phantom,
	title={Phantom: Untargeted Poisoning Attacks on Semi-Supervised Learning},
	author={Knauer, Jonathan and Rieger, Phillip and Fereidooni, Hossein and Sadeghi, Ahmad-Reza},
	booktitle={Proceedings of the 2024 on ACM SIGSAC Conference on Computer and Communications Security},
	pages={615--629},
	year={2024}
}
@InProceedings{Pourkeshavarz2024CVPR,
	author    = {Pourkeshavarz, Mozhgan and Sabokrou, Mohammad and Rasouli, Amir},
	title     = {Adversarial Backdoor Attack by Naturalistic Data Poisoning on Trajectory Prediction in Autonomous Driving},
	booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
	month     = {June},
	year      = {2024},
	pages     = {14885-14894}
}
@InProceedings{Abbasi2024CVPR,
	author    = {Abbasi, Ali and Nooralinejad, Parsa and Pirsiavash, Hamed and Kolouri, Soheil},
	title     = {BrainWash: A Poisoning Attack to Forget in Continual Learning},
	booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
	month     = {June},
	year      = {2024},
	pages     = {24057-24067}
}
@InProceedings{Zhang2024CVPR,
	author    = {Zhang, Jinghuai and Liu, Hongbin and Jia, Jinyuan and Gong, Neil Zhenqiang},
	title     = {Data Poisoning based Backdoor Attacks to Contrastive Learning},
	booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
	month     = {June},
	year      = {2024},
	pages     = {24357-24366}
}
@inproceedings{2017-IJCAI-zhao2017efficient,
	title={Efficient label contamination attacks against black-box learning models.},
	author={Zhao, Mengchen and An, Bo and Gao, Wei and Zhang, Teng},
	booktitle={IJCAI},
	pages={3945--3951},
	year={2017}
}
@inproceedings{2023-AAAI-yuuntargeted,
	title={Untargeted attack against federated recommendation systems via poisonous item embeddings and the defense},
	author={Yu, Yang and Liu, Qi and Wu, Likang and Yu, Runlong and Yu, Sanshi Lei and Zhang, Zaixi},
	booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
	volume={37},
	number={4},
	pages={4854--4863},
	year={2023}
}
@inproceedings{wang2023analysis,
	title={An analysis of untargeted poisoning attack and defense methods for federated online learning to rank systems},
	author={Wang, Shuyi and Zuccon, Guido},
	booktitle={Proceedings of the 2023 ACM SIGIR International Conference on Theory of Information Retrieval},
	pages={215--224},
	year={2023}
}
%@inproceedings{newell2014practicality,
%	title={On the practicality of integrity attacks on document-level sentiment analysis},
%	author={Newell, Andrew and Potharaju, Rahul and Xiang, Luojie and Nita-Rotaru, Cristina},
%	booktitle={Proceedings of the 2014 Workshop on Artificial Intelligent and Security Workshop},
%	pages={83--93},
%	year={2014}
%}
@inproceedings{2021-GlobalCOM-pwage,
	title={Pwpae: An ensemble framework for concept drift adaptation in iot data streams},
	author={Yang, Li and Manias, Dimitrios Michael and Shami, Abdallah},
	booktitle={2021 IEEE Global Communications Conference (GLOBECOM)},
	pages={01--06},
	year={2021},
	organization={IEEE}
}
%@article{2021-Q1-Diverse-instance-weighting,
%	title={Diverse instance-weighting ensemble based on region drift disagreement for concept drift adaptation},
%	author={Liu, Anjin and Lu, Jie and Zhang, Guangquan},
%	journal={IEEE transactions on neural networks and learning systems},
%	volume={32},
%	number={1},
%	pages={293--307},
%	year={2020},
%	publisher={IEEE}
%}
@article{malekghaini2023deep,
	title={Deep learning for encrypted traffic classification in the face of data drift: An empirical study},
	author={Malekghaini, Navid and Akbari, Elham and Salahuddin, Mohammad A and Limam, Noura and Boutaba, Raouf and Mathieu, Bertrand and Moteau, Stephanie and Tuffin, Stephane},
	journal={Computer Networks},
	volume={225},
	pages={109648},
	year={2023},
	publisher={Elsevier}
}
@article{rabanser2019failing,
	title={Failing loudly: An empirical study of methods for detecting dataset shift},
	author={Rabanser, Stephan and G{\"u}nnemann, Stephan and Lipton, Zachary},
	journal={Advances in Neural Information Processing Systems},
	volume={32},
	year={2019}
}
@article{paleyes2022challenges,
	title={Challenges in deploying machine learning: a survey of case studies},
	author={Paleyes, Andrei and Urma, Raoul-Gabriel and Lawrence, Neil D},
	journal={ACM computing surveys},
	volume={55},
	number={6},
	pages={1--29},
	year={2022},
	publisher={ACM New York, NY}
}
@inproceedings{yang2024recda,
	title={Recda: Concept drift adaptation with representation enhancement for network intrusion detection},
	author={Yang, Shuo and Zheng, Xinran and Li, Jinze and Xu, Jinfeng and Wang, Xingjun and Ngai, Edith CH},
	booktitle={Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
	pages={3818--3828},
	year={2024}
}
@article{li2023trustworthy,
	title={Trustworthy AI: From principles to practices},
	author={Li, Bo and Qi, Peng and Liu, Bo and Di, Shuai and Liu, Jingen and Pei, Jiquan and Yi, Jinfeng and Zhou, Bowen},
	journal={ACM Computing Surveys},
	volume={55},
	number={9},
	pages={1--46},
	year={2023},
	publisher={ACM New York, NY}
}
@article{mozaffari2014systematic,
	title={Systematic poisoning attacks on and defenses for machine learning in healthcare},
	author={Mozaffari-Kermani, Mehran and Sur-Kolay, Susmita and Raghunathan, Anand and Jha, Niraj K},
	journal={IEEE journal of biomedical and health informatics},
	volume={19},
	number={6},
	pages={1893--1905},
	year={2014},
	publisher={IEEE}
}
@inproceedings{lata2024exploring,
	title={Exploring Model Poisoning Attack to Convolutional Neural Network Based Brain Tumor Detection Systems},
	author={Lata, Kusum and Singh, Prashant and Saini, Sandeep},
	booktitle={2024 25th International Symposium on Quality Electronic Design (ISQED)},
	pages={1--7},
	year={2024},
	organization={IEEE}
}
@article{wang2024data,
	title={Data poisoning attacks in intelligent transportation systems: A survey},
	author={Wang, Feilong and Wang, Xin and Ban, Xuegang Jeff},
	journal={Transportation Research Part C: Emerging Technologies},
	volume={165},
	pages={104750},
	year={2024},
	publisher={Elsevier}
}
@article{ovadia2019can,
	title={Can you trust your model's uncertainty? evaluating predictive uncertainty under dataset shift},
	author={Ovadia, Yaniv and Fertig, Emily and Ren, Jie and Nado, Zachary and Sculley, David and Nowozin, Sebastian and Dillon, Joshua and Lakshminarayanan, Balaji and Snoek, Jasper},
	journal={Advances in neural information processing systems},
	volume={32},
	year={2019}
}

@InProceedings{pmlr-v139-koh21a,
	title = 	 {WILDS: A Benchmark of in-the-Wild Distribution Shifts},
	author =       {Koh, Pang Wei and Sagawa, Shiori and Marklund, Henrik and Xie, Sang Michael and Zhang, Marvin and Balsubramani, Akshay and Hu, Weihua and Yasunaga, Michihiro and Phillips, Richard Lanas and Gao, Irena and Lee, Tony and David, Etienne and Stavness, Ian and Guo, Wei and Earnshaw, Berton and Haque, Imran and Beery, Sara M and Leskovec, Jure and Kundaje, Anshul and Pierson, Emma and Levine, Sergey and Finn, Chelsea and Liang, Percy},
	booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
	pages = 	 {5637--5664},
	year = 	 {2021},
	editor = 	 {Meila, Marina and Zhang, Tong},
	volume = 	 {139},
	series = 	 {Proceedings of Machine Learning Research},
	month = 	 {18--24 Jul},
	publisher =    {PMLR}
}
@inproceedings{aaaiYuLZ024,
	title={Online boosting adaptive learning under concept drift for multistream classification},
	author={Yu, En and Lu, Jie and Zhang, Bin and Zhang, Guangquan},
	booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
	volume={38},
	number={15},
	pages={16522--16530},
	year={2024}
}
@misc{Baidu-ImageRecognition,
	title = {{Baidu-ImageRecognition}},
	year         = {2024},
	howpublished = {\url{https://cloud.baidu.com/}},
	note         = {Accessed: 2025-01-06}
}
@article{zhou2022domain,
	title={Domain generalization: A survey},
	author={Zhou, Kaiyang and Liu, Ziwei and Qiao, Yu and Xiang, Tao and Loy, Chen Change},
	journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
	volume={45},
	number={4},
	pages={4396--4415},
	year={2022},
	publisher={IEEE}
}
@article{wang2022generalizing,
	title={Generalizing to unseen domains: A survey on domain generalization},
	author={Wang, Jindong and Lan, Cuiling and Liu, Chang and Ouyang, Yidong and Qin, Tao and Lu, Wang and Chen, Yiqiang and Zeng, Wenjun and Philip, S Yu},
	journal={IEEE transactions on knowledge and data engineering},
	volume={35},
	number={8},
	pages={8052--8072},
	year={2022},
	publisher={IEEE}
}
@inproceedings{2014-Adversarial-Active-Learning,
	author = {Miller, Brad and Kantchelian, Alex and Afroz, Sadia and Bachwani, Rekha and Dauber, Edwin and Huang, Ling and Tschantz, Michael Carl and Joseph, Anthony D. and Tygar, J.D.},
	title = {Adversarial Active Learning},
	year = {2014},
	isbn = {9781450331531},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/2666652.2666656},
	doi = {10.1145/2666652.2666656},
	abstract = {Active learning is an area of machine learning examining strategies for allocation of finite resources, particularly human labeling efforts and to an extent feature extraction, in situations where available data exceeds available resources. In this open problem paper, we motivate the necessity of active learning in the security domain, identify problems caused by the application of present active learning techniques in adversarial settings, and propose a framework for experimentation and implementation of active learning systems in adversarial contexts. More than other contexts, adversarial contexts particularly need active learning as ongoing attempts to evade and confuse classifiers necessitate constant generation of labels for new content to keep pace with adversarial activity. Just as traditional machine learning algorithms are vulnerable to adversarial manipulation, we discuss assumptions specific to active learning that introduce additional vulnerabilities, as well as present vulnerabilities that are amplified in the active learning setting. Lastly, we present a software architecture, Security-oriented Active Learning Testbed (SALT), for the research and implementation of active learning applications in adversarial contexts.},
	booktitle = {Proceedings of the 2014 Workshop on Artificial Intelligent and Security Workshop},
	pages = {3–14},
	numpages = {12},
	keywords = {active learning, human in the loop, secure machine learning},
	location = {Scottsdale, Arizona, USA},
	series = {AISec '14}
}
@article{chen2018detecting,
	title={Detecting backdoor attacks on deep neural networks by activation clustering},
	author={Chen, Bryant and Carvalho, Wilka and Baracaldo, Nathalie and Ludwig, Heiko and Edwards, Benjamin and Lee, Taesung and Molloy, Ian and Srivastava, Biplav},
	journal={arXiv preprint arXiv:1811.03728},
	year={2018}
}
@inproceedings{FP,
	title={Fine-pruning: Defending against backdooring attacks on deep neural networks},
	author={Liu, Kang and Dolan-Gavitt, Brendan and Garg, Siddharth},
	booktitle={International symposium on research in attacks, intrusions, and defenses},
	pages={273--294},
	year={2018},
	organization={Springer}
}
@inproceedings{FT,
	title={Neural trojans},
	author={Liu, Yuntao and Xie, Yang and Srivastava, Ankur},
	booktitle={2017 IEEE International Conference on Computer Design (ICCD)},
	pages={45--48},
	year={2017},
	organization={IEEE}
}
@inproceedings{2023-ICCV-Trigger-Detect,
	title={Detecting backdoors during the inference stage based on corruption robustness consistency},
	author={Liu, Xiaogeng and Li, Minghui and Wang, Haoyu and Hu, Shengshan and Ye, Dengpan and Jin, Hai and Wu, Libing and Xiao, Chaowei},
	booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
	pages={16363--16372},
	year={2023}
}
@inproceedings{DFP,
	title={On the Difficulty of Defending Contrastive Learning against Backdoor Attacks},
	author={Li, Changjiang and Pang, Ren and Cao, Bochuan and Xi, Zhaohan and Chen, Jinghui and Ji, Shouling and Wang, Ting},
	booktitle={33rd USENIX Security Symposium (USENIX Security 24)},
	pages={2901--2918},
	year={2024}
}
@article{hoang2024rip,
	title={RIP: A Simple Black-box Attack on Continual Test-time Adaptation},
	author={Hoang, Trung-Hieu and Vo, Duc Minh and Do, Minh N},
	journal={arXiv preprint arXiv:2412.01154},
	year={2024}
}
@inproceedings{pang2020tale,
	title={A tale of evil twins: Adversarial inputs versus poisoned models},
	author={Pang, Ren and Shen, Hua and Zhang, Xinyang and Ji, Shouling and Vorobeychik, Yevgeniy and Luo, Xiapu and Liu, Alex and Wang, Ting},
	booktitle={Proceedings of the 2020 ACM SIGSAC conference on computer and communications security},
	pages={85--99},
	year={2020}
}
@inproceedings{9671964,
	author={Liu, Guanxiong and Khalil, Issa and Khreishah, Abdallah and Phan, NhatHai},
	booktitle={2021 IEEE International Conference on Big Data (Big Data)}, 
	title={A Synergetic Attack against Neural Network Classifiers combining Backdoor and Adversarial Examples}, 
	year={2021},
	volume={},
	number={},
	pages={834-846},
	keywords={Training;Toxicology;Perturbation methods;Computational modeling;Image processing;Training data;Artificial neural networks;Neural networks;adversarial attack;Trojan attack},
	doi={10.1109/BigData52589.2021.9671964}}
@inproceedings{naseri2024badvfl,
	title={Badvfl: Backdoor attacks in vertical federated learning},
	author={Naseri, Mohammad and Han, Yufei and De Cristofaro, Emiliano},
	booktitle={2024 IEEE Symposium on Security and Privacy (SP)},
	pages={2013--2028},
	year={2024},
	organization={IEEE}
}
@inproceedings{krauss2024automatic,
	title={Automatic adversarial adaption for stealthy poisoning attacks in federated learning},
	author={Krau{\ss}, Torsten and K{\"o}nig, Jan and Dmitrienko, Alexandra and Kanzow, Christian},
	booktitle={To appear soon at the Network and Distributed System Security Symposium (NDSS)},
	year={2024}
}
@inproceedings{lyu2023poisoning,
	title={Poisoning with cerberus: Stealthy and colluded backdoor attack against federated learning},
	author={Lyu, Xiaoting and Han, Yufei and Wang, Wei and Liu, Jingkai and Wang, Bin and Liu, Jiqiang and Zhang, Xiangliang},
	booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
	volume={37},
	number={7},
	pages={9020--9028},
	year={2023}
}
@article{tan2023collusive,
	title={Collusive model poisoning attack in decentralized federated learning},
	author={Tan, Shouhong and Hao, Fengrui and Gu, Tianlong and Li, Long and Liu, Ming},
	journal={IEEE Transactions on Industrial Informatics},
	year={2023},
	publisher={IEEE}
}
@misc{GriftHorseMalware,
	author = {zimperium},
	year={2021},
	title = {GriftHorse Android Trojan Steals Millions from Over 10 Million Victims Globally},
	howpublished = {\url{https://www.zimperium.com/blog/grifthorse-android-trojan-steals-millions-from-over-10-million-victims-globally/}}    
}
@article{2024-Q1-Recommender-Systems-Survey-of-Poisoning-Attacks,
	author = {Nguyen, Thanh Toan and Quoc Viet hung, Nguyen and Nguyen, Thanh Tam and Huynh, Thanh Trung and Nguyen, Thanh Thi and Weidlich, Matthias and Yin, Hongzhi},
	title = {Manipulating Recommender Systems: A Survey of Poisoning Attacks and Countermeasures},
	year = {2024},
	issue_date = {January 2025},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {57},
	number = {1},
	issn = {0360-0300},
	url = {https://doi.org/10.1145/3677328},
	doi = {10.1145/3677328},
	abstract = {Recommender systems have become an integral part of online services due to their ability to help users locate specific information in a sea of data. However, existing studies show that some recommender systems are vulnerable to poisoning attacks, particularly those that involve learning schemes. A poisoning attack is where an adversary injects carefully crafted data into the process of training a model with the goal of manipulating the system’s final recommendations. Based on recent advancements in artificial intelligence (AI), such attacks have gained importance recently. At present, we do not have a full and clear picture of why adversaries mount such attacks, nor do we have comprehensive knowledge of the full capacity to which such attacks can undermine a model or the impacts that might have. While numerous countermeasures to poisoning attacks have been developed, they have not yet been systematically linked to the properties of the attacks. Consequently, assessing the respective risks and potential success of mitigation strategies is difficult, if not impossible. This survey aims to fill this gap by primarily focusing on poisoning attacks and their countermeasures. This is in contrast to prior surveys that mainly focus on attacks and their detection methods. Through an exhaustive literature review, we provide a novel taxonomy for poisoning attacks, formalise its dimensions, and accordingly organise 31 attacks described in the literature. Further, we review 43 countermeasures to detect and/or prevent poisoning attacks, evaluating their effectiveness against specific types of attacks. This comprehensive survey should serve as a point of reference for protecting recommender systems against poisoning attacks. The article concludes with a discussion on open issues in the field and impactful directions for future research. A rich repository of resources associated with poisoning attacks is available at .},
	journal = {ACM Comput. Surv.},
	month = oct,
	articleno = {3},
	numpages = {39},
	keywords = {Trustworthy recommender systems, trustworthy AI, poisoning attacks, poison attacks, model corruption}
}
@article{2023-CCF-B-Adversarial-concept-drift-detection-under-poisoning-attacks,
	title={Adversarial concept drift detection under poisoning attacks for robust data stream mining},
	author={Korycki, {\L}ukasz and Krawczyk, Bartosz},
	journal={Machine Learning},
	volume={112},
	number={10},
	pages={4013--4048},
	year={2023},
	publisher={Springer}
}
@inproceedings{park2016active,
	title={An active learning method for data streams with concept drift},
	author={Park, Cheong Hee and Kang, Youngsoon},
	booktitle={2016 IEEE International Conference on Big Data (Big Data)},
	pages={746--752},
	year={2016},
	organization={IEEE}
}
@article{vzliobaite2013active,
	title={Active learning with drifting streaming data},
	author={{\v{Z}}liobait{\.e}, Indr{\.e} and Bifet, Albert and Pfahringer, Bernhard and Holmes, Geoffrey},
	journal={IEEE transactions on neural networks and learning systems},
	volume={25},
	number={1},
	pages={27--39},
	year={2013},
	publisher={IEEE}
}
@inproceedings{2012-ICML-Poisoning-attacks-against-support-vector-machines,
	author = {Biggio, Battista and Nelson, Blaine and Laskov, Pavel},
	title = {Poisoning attacks against support vector machines},
	year = {2012},
	isbn = {9781450312851},
	publisher = {Omnipress},
	address = {Madison, WI, USA},
	abstract = {We investigate a family of poisoning attacks against Support Vector Machines (SVM). Such attacks inject specially crafted training data that increases the SVM's test error. Central to the motivation for these attacks is the fact that most learning algorithms assume that their training data comes from a natural or well-behaved distribution. However, this assumption does not generally hold in security-sensitive settings. As we demonstrate, an intelligent adversary can, to some extent, predict the change of the SVM's decision function due to malicious input and use this ability to construct malicious data.The proposed attack uses a gradient ascent strategy in which the gradient is computed based on properties of the SVM's optimal solution. This method can be kernelized and enables the attack to be constructed in the input space even for non-linear kernels. We experimentally demonstrate that our gradient ascent procedure reliably identifies good local maxima of the non-convex validation error surface, which significantly increases the classifier's test error.},
	booktitle = {Proceedings of the 29th International Coference on International Conference on Machine Learning},
	pages = {1467–1474},
	numpages = {8},
	location = {Edinburgh, Scotland},
	series = {ICML'12}
}
@article{ZLIOBAITE2015240,
	title = {Towards cost-sensitive adaptation: When is it worth updating your predictive model?},
	journal = {Neurocomputing},
	volume = {150},
	pages = {240-249},
	year = {2015},
	note = {Bioinspired and knowledge based techniques and applications The Vitality of Pattern Recognition and Image Analysis Data Stream Classification and Big Data Analytics},
	issn = {0925-2312},
	doi = {https://doi.org/10.1016/j.neucom.2014.05.084},
	url = {https://www.sciencedirect.com/science/article/pii/S0925231214012971},
	author = {Indrė Žliobaitė and Marcin Budka and Frederic Stahl},
	keywords = {Evolving data streams, Concept drift, Evaluation, Cost-sensitive adaptation, Utility of data mining},
	abstract = {Our digital universe is rapidly expanding, more and more daily activities are digitally recorded, data arrives in streams, it needs to be analyzed in real time and may evolve over time. In the last decade many adaptive learning algorithms and prediction systems, which can automatically update themselves with the new incoming data, have been developed. The majority of those algorithms focus on improving the predictive performance and assume that model update is always desired as soon as possible and as frequently as possible. In this study we consider potential model update as an investment decision, which, as in the financial markets, should be taken only if a certain return on investment is expected. We introduce and motivate a new research problem for data streams – cost-sensitive adaptation. We propose a reference framework for analyzing adaptation strategies in terms of costs and benefits. Our framework allows us to characterize and decompose the costs of model updates, and to asses and interpret the gains in performance due to model adaptation for a given learning algorithm on a given prediction task. Our proof-of-concept experiment demonstrates how the framework can aid in analyzing and managing adaptation decisions in the chemical industry.}
}
@article{SHAKER2015250,
	title = {Recovery analysis for adaptive learning from non-stationary data streams: Experimental design and case study},
	journal = {Neurocomputing},
	volume = {150},
	pages = {250-264},
	year = {2015},
	note = {Bioinspired and knowledge based techniques and applications The Vitality of Pattern Recognition and Image Analysis Data Stream Classification and Big Data Analytics},
	issn = {0925-2312},
	doi = {https://doi.org/10.1016/j.neucom.2014.09.076},
	url = {https://www.sciencedirect.com/science/article/pii/S0925231214013216},
	author = {Ammar Shaker and Eyke Hüllermeier},
	keywords = {Machine learning, Data streams, Concept drift, Supervised learning, Regression, Classification},
	abstract = {The extension of machine learning methods from static to dynamic environments has received increasing attention in recent years; in particular, a large number of algorithms for learning from so-called data streams has been developed. An important property of dynamic environments is non-stationarity, i.e., the assumption of an underlying data generating process that may change over time. Correspondingly, the ability to properly react to so-called concept change is considered as an important feature of learning algorithms. In this paper, we propose a new type of experimental analysis, called recovery analysis, which is aimed at assessing the ability of a learner to discover a concept change quickly, and to take appropriate measures to maintain the quality and generalization performance of the model. We develop recovery analysis for two types of supervised learning problems, namely classification and regression. Moreover, as a practical application, we make use of recovery analysis in order to compare model-based and instance-based approaches to learning on data streams.}
}
@article{settles2009active,
	title={Active learning literature survey},
	author={Settles, Burr},
	year={2009},
	publisher={University of Wisconsin-Madison Department of Computer Sciences}
}
@inproceedings{NIPS2017_8ca8da41,
	author = {Konyushkova, Ksenia and Sznitman, Raphael and Fua, Pascal},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
	pages = {},
	publisher = {Curran Associates, Inc.},
	title = {Learning Active Learning from Data},
	url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/8ca8da41fe1ebc8d3ca31dc14f5fc56c-Paper.pdf},
	volume = {30},
	year = {2017}
}
@article{Hsu_Lin_2015, 
	title={Active Learning by Learning}, volume={29}, url={https://ojs.aaai.org/index.php/AAAI/article/view/9597}, DOI={10.1609/aaai.v29i1.9597}, abstractNote={ &lt;p&gt; Pool-based active learning is an important technique that helps reduce labeling efforts within a pool of unlabeled instances. Currently, most pool-based active learning strategies are constructed based on some human-designed philosophy; that is, they reflect what human beings assume to be “good labeling questions.” However, while such human-designed philosophies can be useful on specific data sets, it is often difficult to establish the theoretical connection of those philosophies to the true learning performance of interest. In addition, given that a single human-designed philosophy is unlikely to work on all scenarios, choosing and blending those strategies under different scenarios is an important but challenging practical task. This paper tackles this task by letting the machines adaptively “learn” from the performance of a set of given strategies on a particular data set. More specifically, we design a learning algorithm that connects active learning with the well-known multi-armed bandit problem. Further, we postulate that, given an appropriate choice for the multi-armed bandit learner, it is possible to estimate the performance of different strategies on the fly. Extensive empirical studies of the resulting ALBL algorithm confirm that it performs better than state-of-the-art strategies and a leading blending algorithm for active learning, all of which are based on human-designed philosophy. &lt;/p&gt; }, number={1}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Hsu, Wei-Ning and Lin, Hsuan-Tien}, year={2015}, month={Feb.} }
@article{cacciarelli2024active,
	title={Active learning for data streams: a survey},
	author={Cacciarelli, Davide and Kulahci, Murat},
	journal={Machine Learning},
	volume={113},
	number={1},
	pages={185--239},
	year={2024},
	publisher={Springer}
}
@inproceedings{pmlr-v20-biggio11,
	title={Support vector machines under adversarial label noise},
	author={Biggio, Battista and Nelson, Blaine and Laskov, Pavel},
	booktitle={Asian conference on machine learning},
	pages={97--112},
	year={2011},
	organization={PMLR}
}
@inproceedings {247652,
	author = {Minghong Fang and Xiaoyu Cao and Jinyuan Jia and Neil Gong},
	title = {Local Model Poisoning Attacks to {Byzantine-Robust} Federated Learning},
	booktitle = {29th USENIX Security Symposium (USENIX Security 20)},
	year = {2020},
	isbn = {978-1-939133-17-5},
	pages = {1605--1622},
	url = {https://www.usenix.org/conference/usenixsecurity20/presentation/fang},
	publisher = {USENIX Association},
	month = aug
}
@inproceedings{10.1145/3128572.3140451,
	author = {Mu\~{n}oz-Gonz\'{a}lez, Luis and Biggio, Battista and Demontis, Ambra and Paudice, Andrea and Wongrassamee, Vasin and Lupu, Emil C. and Roli, Fabio},
	title = {Towards Poisoning of Deep Learning Algorithms with Back-gradient Optimization},
	year = {2017},
	isbn = {9781450352024},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3128572.3140451},
	doi = {10.1145/3128572.3140451},
	abstract = {A number of online services nowadays rely upon machine learning to extract valuable information from data collected in the wild. This exposes learning algorithms to the threat of data poisoning, i.e., a coordinate attack in which a fraction of the training data is controlled by the attacker and manipulated to subvert the learning process. To date, these attacks have been devised only against a limited class of binary learning algorithms, due to the inherent complexity of the gradient-based procedure used to optimize the poisoning points (a.k.a. adversarial training examples). In this work, we first extend the definition of poisoning attacks to multiclass problems. We then propose a novel poisoning algorithm based on the idea of back-gradient optimization, i.e., to compute the gradient of interest through automatic differentiation, while also reversing the learning procedure to drastically reduce the attack complexity. Compared to current poisoning strategies, our approach is able to target a wider class of learning algorithms, trained with gradient-based procedures, including neural networks and deep learning architectures. We empirically evaluate its effectiveness on several application examples, including spam filtering, malware detection, and handwritten digit recognition. We finally show that, similarly to adversarial test examples, adversarial training examples can also be transferred across different learning algorithms.},
	booktitle = {Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security},
	pages = {27–38},
	numpages = {12},
	keywords = {training data poisoning, deep learning, adversarial machine learning, adversarial examples},
	location = {Dallas, Texas, USA},
	series = {AISec '17}
}
@InProceedings{Ledda_2023_ICCV,
	author    = {Ledda, Emanuele and Angioni, Daniele and Piras, Giorgio and Fumera, Giorgio and Biggio, Battista and Roli, Fabio},
	title     = {Adversarial Attacks Against Uncertainty Quantification},
	booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) Workshops},
	month     = {October},
	year      = {2023},
	pages     = {4599-4608}
}
@article{NEURIPS2023_16e4be78,
	title={Explaining predictive uncertainty with information theoretic shapley values},
	author={Watson, David and O'Hara, Joshua and Tax, Niek and Mudd, Richard and Guy, Ido},
	journal={Advances in Neural Information Processing Systems},
	volume={36},
	pages={7330--7350},
	year={2023}
}
@InProceedings{Abbasi_2024_CVPR,
	author    = {Abbasi, Ali and Nooralinejad, Parsa and Pirsiavash, Hamed and Kolouri, Soheil},
	title     = {BrainWash: A Poisoning Attack to Forget in Continual Learning},
	booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
	month     = {June},
	year      = {2024},
	pages     = {24057-24067}
}
@inproceedings{2023-AAAI-surrogate-model-for-adversarial-attack,
	title={Training meta-surrogate model for transferable adversarial attack},
	author={Qin, Yunxiao and Xiong, Yuanhao and Yi, Jinfeng and Hsieh, Cho-Jui},
	booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
	volume={37},
	number={8},
	pages={9516--9524},
	year={2023}
}
@inproceedings{2017-ASIACCS-Black-Box-Attack,
	title={Practical black-box attacks against machine learning},
	author={Papernot, Nicolas and McDaniel, Patrick and Goodfellow, Ian and Jha, Somesh and Celik, Z Berkay and Swami, Ananthram},
	booktitle={Proceedings of the 2017 ACM on Asia conference on computer and communications security},
	pages={506--519},
	year={2017}
}
@inproceedings{shrikumar2017learning,
	title={Learning important features through propagating activation differences},
	author={Shrikumar, Avanti and Greenside, Peyton and Kundaje, Anshul},
	booktitle={International conference on machine learning},
	pages={3145--3153},
	year={2017},
	organization={PMlR}
}
@article{lundberg2017unified,
	title={A unified approach to interpreting model predictions},
	author={Lundberg, Scott M and Lee, Su-In},
	journal={Advances in neural information processing systems},
	volume={30},
	year={2017}
}
@article{ganguly2023online,
	title={Online federated learning via non-stationary detection and adaptation amidst concept drift},
	author={Ganguly, Bhargav and Aggarwal, Vaneet},
	journal={IEEE/ACM Transactions on Networking},
	volume={32},
	number={1},
	pages={643--653},
	year={2023},
	publisher={IEEE}
}
@article{minh2022explainable,
	title={Explainable artificial intelligence: a comprehensive review},
	author={Minh, Dang and Wang, H Xiang and Li, Y Fen and Nguyen, Tan N},
	journal={Artificial Intelligence Review},
	pages={1--66},
	year={2022},
	publisher={Springer}
}
@article{angelov2021explainable,
	title={Explainable artificial intelligence: an analytical review},
	author={Angelov, Plamen P and Soares, Eduardo A and Jiang, Richard and Arnold, Nicholas I and Atkinson, Peter M},
	journal={Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
	volume={11},
	number={5},
	pages={e1424},
	year={2021},
	publisher={Wiley Online Library}
}
@article{ahmed2022artificial,
	title={From artificial intelligence to explainable artificial intelligence in industry 4.0: a survey on what, how, and where},
	author={Ahmed, Imran and Jeon, Gwanggil and Piccialli, Francesco},
	journal={IEEE Transactions on Industrial Informatics},
	volume={18},
	number={8},
	pages={5031--5042},
	year={2022},
	publisher={IEEE}
}
@article{ali2023explainable,
	title={Explainable Artificial Intelligence (XAI): What we know and what is left to attain Trustworthy Artificial Intelligence},
	author={Ali, Sajid and Abuhmed, Tamer and El-Sappagh, Shaker and Muhammad, Khan and Alonso-Moral, Jose M and Confalonieri, Roberto and Guidotti, Riccardo and Del Ser, Javier and D{\'\i}az-Rodr{\'\i}guez, Natalia and Herrera, Francisco},
	journal={Information fusion},
	volume={99},
	pages={101805},
	year={2023},
	publisher={Elsevier}
}
@article{lundberg2020local,
	title={From local explanations to global understanding with explainable AI for trees},
	author={Lundberg, Scott M and Erion, Gabriel and Chen, Hugh and DeGrave, Alex and Prutkin, Jordan M and Nair, Bala and Katz, Ronit and Himmelfarb, Jonathan and Bansal, Nisha and Lee, Su-In},
	journal={Nature machine intelligence},
	volume={2},
	number={1},
	pages={56--67},
	year={2020},
	publisher={Nature Publishing Group}
}
@inproceedings{ribeiro2016should,
	title={" Why should i trust you?" Explaining the predictions of any classifier},
	author={Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
	booktitle={Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining},
	pages={1135--1144},
	year={2016}
}
@inproceedings{sundararajan2017axiomatic,
	title={Axiomatic attribution for deep networks},
	author={Sundararajan, Mukund and Taly, Ankur and Yan, Qiqi},
	booktitle={International conference on machine learning},
	pages={3319--3328},
	year={2017},
	organization={PMLR}
}
@article{mirkovic2004taxonomy,
	title={A taxonomy of DDoS attack and DDoS defense mechanisms},
	author={Mirkovic, Jelena and Reiher, Peter},
	journal={ACM SIGCOMM Computer Communication Review},
	volume={34},
	number={2},
	pages={39--53},
	year={2004},
	publisher={ACM New York, NY, USA}
}
@inproceedings{kong2023peeling,
	title={Peeling the onion: Hierarchical reduction of data redundancy for efficient vision transformer training},
	author={Kong, Zhenglun and Ma, Haoyu and Yuan, Geng and Sun, Mengshu and Xie, Yanyue and Dong, Peiyan and Meng, Xin and Shen, Xuan and Tang, Hao and Qin, Minghai and others},
	booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
	volume={37},
	number={7},
	pages={8360--8368},
	year={2023}
}
@inproceedings{yuan2022optical,
	title={Optical flow training under limited label budget via active learning},
	author={Yuan, Shuai and Sun, Xian and Kim, Hannah and Yu, Shuzhi and Tomasi, Carlo},
	booktitle={European conference on computer vision},
	pages={410--427},
	year={2022},
	organization={Springer}
}
@article{hacohen2023select,
	title={How to select which active learning strategy is best suited for your specific problem and budget},
	author={Hacohen, Guy and Weinshall, Daphna},
	journal={Advances in Neural Information Processing Systems},
	volume={36},
	pages={13395--13407},
	year={2023}
}
%@inproceedings{shao2019learning,
%	title={Learning to sample: an active learning framework},
%	author={Shao, Jingyu and Wang, Qing and Liu, Fangbing},
%	booktitle={2019 IEEE international conference on data mining (ICDM)},
%	pages={538--547},
%	year={2019},
%	organization={IEEE}
%}
%@inproceedings{li2022targeted,
%	title={Targeted data poisoning attacks against continual learning neural networks},
%	author={Li, Huayu and Ditzler, Gregory},
%	booktitle={2022 International Joint Conference on Neural Networks (IJCNN)},
%	pages={1--8},
%	year={2022},
%	organization={IEEE}
%}
@inproceedings{han2023data,
	title={Data poisoning attack aiming the vulnerability of continual learning},
	author={Han, Gyojin and Choi, Jaehyun and Hong, Hyeong Gwon and Kim, Junmo},
	booktitle={2023 IEEE International Conference on Image Processing (ICIP)},
	pages={1905--1909},
	year={2023},
	organization={IEEE}
}
@article{guo2024persistent,
	title={Persistent Backdoor Attacks in Continual Learning},
	author={Guo, Zhen and Kumar, Abhinav and Tourani, Reza},
	journal={arXiv preprint arXiv:2409.13864},
	year={2024}
}
@article{de2021continual,
	title={A continual learning survey: Defying forgetting in classification tasks},
	author={De Lange, Matthias and Aljundi, Rahaf and Masana, Marc and Parisot, Sarah and Jia, Xu and Leonardis, Ale{\v{s}} and Slabaugh, Gregory and Tuytelaars, Tinne},
	journal={IEEE transactions on pattern analysis and machine intelligence},
	volume={44},
	number={7},
	pages={3366--3385},
	year={2021},
	publisher={IEEE}
}
@article{wang2024comprehensive,
	title={A comprehensive survey of continual learning: Theory, method and application},
	author={Wang, Liyuan and Zhang, Xingxing and Su, Hang and Zhu, Jun},
	journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
	year={2024},
	publisher={IEEE}
}
@article{wickramasinghe2023continual,
	title={Continual learning: A review of techniques, challenges, and future directions},
	author={Wickramasinghe, Buddhi and Saha, Gobinda and Roy, Kaushik},
	journal={IEEE Transactions on Artificial Intelligence},
	volume={5},
	number={6},
	pages={2526--2546},
	year={2023},
	publisher={IEEE}
}
@inproceedings{miller2014adversarial,
	title={Adversarial active learning},
	author={Miller, Brad and Kantchelian, Alex and Afroz, Sadia and Bachwani, Rekha and Dauber, Edwin and Huang, Ling and Tschantz, Michael Carl and Joseph, Anthony D and Tygar, J Doug},
	booktitle={Proceedings of the 2014 workshop on artificial intelligent and security workshop},
	pages={3--14},
	year={2014}
}
@inproceedings{zhao2012sampling,
	title={Sampling attack against active learning in adversarial environment},
	author={Zhao, Wentao and Long, Jun and Yin, Jianping and Cai, Zhiping and Xia, Geming},
	booktitle={Modeling Decisions for Artificial Intelligence: 9th International Conference, MDAI 2012, Girona, Catalonia, Spain, November 21-23, 2012. Proceedings 9},
	pages={222--233},
	year={2012},
	organization={Springer}
}
@article{sethi2018handling,
	title={Handling adversarial concept drift in streaming data},
	author={Sethi, Tegjyot Singh and Kantardzic, Mehmed},
	journal={Expert systems with applications},
	volume={97},
	pages={18--40},
	year={2018},
	publisher={Elsevier}
}
@inproceedings{apruzzese2024adversarial,
	title={When adversarial perturbations meet concept drift: an exploratory analysis on ml-nids},
	author={Apruzzese, Giovanni and Fass, Aurore and Pierazzi, Fabio},
	booktitle={Proceedings of the 2024 Workshop on Artificial Intelligence and Security},
	pages={149--160},
	year={2024}
}
@article{settles2009active,
	title={Active learning literature survey},
	author={Settles, Burr},
	year={2009},
	publisher={University of Wisconsin-Madison Department of Computer Sciences}
}
@article{ren2021survey,
	title={A survey of deep active learning},
	author={Ren, Pengzhen and Xiao, Yun and Chang, Xiaojun and Huang, Po-Yao and Li, Zhihui and Gupta, Brij B and Chen, Xiaojiang and Wang, Xin},
	journal={ACM computing surveys (CSUR)},
	volume={54},
	number={9},
	pages={1--40},
	year={2021},
	publisher={ACM New York, NY}
}
@article{ganguly2023online,
	title={Online federated learning via non-stationary detection and adaptation amidst concept drift},
	author={Ganguly, Bhargav and Aggarwal, Vaneet},
	journal={IEEE/ACM Transactions on Networking},
	volume={32},
	number={1},
	pages={643--653},
	year={2023},
	publisher={IEEE}
}