\section{Related Work}
\label{sec: Related work}
% 参考描述1
%In the recent past, a large number of poisoning attacks have been proposed.
%In the following, we discuss the approaches that are most relevant to this work.
% 参考描述2-2021-Usenix-Double-cross
%\pandora fails into the broad categroy of poisoning attacks.
%The attackers inject some poisoned samples to sabotage the prediction performance of the victim model at test time.
%In the following, we summarize the similarities and differences between \pandora and other related poisoning attacks.
%\pandora, a type of poisoning attack, involve injecting poisoned samples to compromise the victim model's test-time performance.
%Below, we summarize the key similarities and differences between \pandora and other poisoning attacks.
%
%\pandora injects poisoned samples to compromise the victim model's performance of concept drift adaptation.
%Below, we summarize key similarities and differences between \pandora and other poisoning attacks.
\textbf{Poisoning Attacks Against Active Learning}.
Zhao et al.~\cite{zhao2012sampling} investigated the security of sampling strategies in active learning. 
However, their approach assumes that attackers can remove samples from the testing data, preventing the victim model from collecting them. 
In contrast, attackers typically do not have control over the victim model’s data collection process.
Miller et al.~\cite{miller2014adversarial} discussed adversarial threats in active learning, highlighting the risk of manipulation in the sample selection process. However, their attack is indiscriminate and lacks stealth.
Vicarte et al.~\cite{2021-Usenix-active-learning-backdoor} proposed a backdoor attack against active learning, leading to misclassification of specific attack targets.
Nevertheless, their method requires embedding triggers into the attack targets, which introduces considerable overhead during the concept drift process.
This overhead arises from the need to continuously update the triggers to keep pace with the evolving victim model throughout the adaptation.
In contrast, our \pandora requires no modifications to the attack targets.

\textbf{Adversarial Concept Drift}.
Korycki et al.~\cite{2023-CCF-B-Adversarial-concept-drift-detection-under-poisoning-attacks} pointed out that existing drift detectors cannot distinguish between real and adversarial concept drift. 
Their proposed adversarial concept drift scenario assumes a strong adversary capable of directly manipulating the victim model’s training dataset, which contrasts with our setting, where only selected samples are labeled and used for training in CDA-AL.
Apruzzese et al.~\cite{apruzzese2024adversarial} analyzed the impact of adversarial perturbations occurring simultaneously with real concept drift in the context of intrusion detection.
However, their study primarily focuses on the intrusion detection scenario, with limited analysis and validation in other domains.
Moreover, their attack methods are limited to the problem space and do not account for the influence of feature space perturbations.
In addition, they typically assume that the victim model does not utilize data filtering mechanisms such as active learning.

%\textcolor{red}{Despite these advantages, previous studies have revealed that the concept drift adaptation process may be vulnerable to poisoning attacks.
%	For instance, prior work by Korycki et al.~\cite{2023-CCF-B-Adversarial-concept-drift-detection-under-poisoning-attacks} demonstrated that injecting poisoned samples into the victim model’s training data can compromise the performance.
%	However, their approach relies on the strong assumption that attackers can arbitrarily inject poisoned samples into the victim model’s training data~\cite{2022-ACM-Computing-Survey-Threats-to-training}.
%	In real-world scenarios, attackers typically have access only to the testing data and lack direct control over the training data.
%	Furthermore, in CDA-AL methods~\cite{2023-Usenix-chenyizhen,park2016active,vzliobaite2013active}, only high-uncertainty samples are manually reviewed and labeled before being added to the training set. 
%	As a result, the risk of poisoning attacks in CDA-AL, where adversaries cannot freely inject training data, remains underexplored.}
%
%\textcolor{blue}{Moreover, existing poisoning attacks on concept drift adaptation degrade overall model performance and lack stealth~\cite{2023-CCF-B-Adversarial-concept-drift-detection-under-poisoning-attacks}. In contrast, \pandora selectively consumes part of the labeling budget for its target, preserving the victim model’s overall performance and enhancing attack stealth.(\textcolor{red}{\textbf{don't mention ~\cite{2023-CCF-B-Adversarial-concept-drift-detection-under-poisoning-attacks}, move to related work}})}
%\textcolor{blue}{Attack-baseline analysis}


%\noindent \textbf{Poisoning Attacks.} 
%While poisoning attacks have been extensively studied~\cite{2024-CCS-Phantom,2023-AAAI-yuuntargeted,wang2023analysis,2018-NIPS-Poison-frogs,2018-Usenix-generalized-transferability,2024-TIFS-Backdoor-Contrastive-Learning,2023-TIFS-Backdoor-Image-Encryption,2024-TIFS-Backdoor-Minimalism-is-King,2023-TIFS-person-re-identification-backdoor}, research on CDA-AL poisoning is limited.
%Recent studies on test-time poisoning~\cite{2024-SP-Test-time-poisoning-attacks} demonstrate that surrogate model poisoned samples can degrade victim model performance.
%These studies focus on self-supervised models under white-box threat scenarios and do not consider situations where poisoned samples may face manual inspection.
%Hoang et al.~\cite{hoang2024rip} extended this work to non-targeted attacks under black-box settings, assuming no manual inspection by model owners. 
%Vicarte et al.~\cite{2021-Usenix-active-learning-backdoor} explored test-time poisoning in active learning, focusing on targeted attacks like backdoors, but their approach relies on triggers and lacks optimization for stealth and cost efficiency.
%In contrast, our \pandora accounts for manual test data inspection and achieves effective results in black-box and gray-box scenarios. 
%It also avoids modifying attack targets, reducing costs while maintaining high effectiveness.

%\noindent \textbf{Multi-Attacker Interaction.} 
%Collaborative attacks by multiple attackers are common in distributed learning scenarios like federated learning~\cite{naseri2024badvfl,krauss2024automatic,lyu2023poisoning,tan2023collusive}.
%However, research on multi-attacker collaborative attacks in centralized learning systems remains limited.
%By evaluating the \pandora on APIGraph, we demonstrate the significance of attacker collaboration in improving attack effectiveness.
%Pang et al.~\cite{pang2020tale} suggest combining adversarial examples with poisoning attacks amplifies their impact on deep learning systems.
%Building on this, Liu et al.\cite{9671964} further explored a synergetic attack against neural network classifiers combining backdoor and adversarial examples.
%However, the above studies lack validation in evolving models.
%Our findings reveal that untargeted poisoning operations undermine the stealthiness of targeted poisoning attacks in concept drift adaptation scenarios.
%To enable effective collaboration in evolving models, we propose an attack negotiation method for multiple attack modes and diverse targets, reducing costs while meeting varied attack requirements at different time.
%Untargeted poisoning attack is the most intuitive kind of attacks.
%Untargeted poisoning attacks do not have a speciic target class.
%The goal of adversary is declining the overall performance of the victim model, such as classification accuracy.
%Most untargeted poisoning attacks occur during the model training phase~\cite{2024-CCS-Phantom,2017-IJCAI-zhao2017efficient,2017-NDSS-yang2017fake,2023-AAAI-yuuntargeted,wang2023analysis,newell2014practicality}.
%However, during the concept drift adaptation process, the training and testing phases often alternate with each other.
%As a result, the poisoning attack scenarios of concept drift adaptation are different from those of most existing untargeted poisoning attacks.
%The closest work to our attack is test-time poisoning ttacks against test-time adaptation models~\cite{2024-SP-Test-time-poisoning-attacks}.
%They study the potential risk of poisoning attacks during test phases and demonstrated that poisoned samples generated by surrogate model could effectively degrade the performance of the victim model.
%However, their work primarily focuses on untargeted attack, leaving a gap in understanding how an attacker conduct targeted poisoning attacks while maintaining the performance of the main task.
%Furthermore, their work discusses targeted and untargeted poisoning attacks separately, lacking an analysis of the relationship between these two types of attacks.
%Untargeted poisoning attacks aim to degrade the overall performance of the victim model, such as classification accuracy, without targeting specific classes.
%Most occur during the training phase~\cite{2024-CCS-Phantom,2017-IJCAI-zhao2017efficient,2023-AAAI-yuuntargeted,wang2023analysis,newell2014practicality}.
%However, in concept drift adaptation, training and testing phases alternate, creating distinct attack scenarios.
%The closest related work is on test-time poisoning attacks against test-time adaptation models~\cite{2024-SP-Test-time-poisoning-attacks}, which show that surrogate-generated poisoned samples can degrade victim model performance.
%However, this work focuses primarily on untargeted attacks, leaving a gap in understanding targeted attacks that preserve main task performance and lacking an analysis of the relationship between targeted and untargeted attacks.

%\noindent \textbf{Targeted Poisoning attacks.}
%Unlike untargeted poisoning attacks, targeted poisoning attacks place greater emphasis on the specific class of incorrect predictions~\cite{2018-NIPS-Poison-frogs,2018-Usenix-generalized-transferability,2013-Usenix-Pollution-attacks,2024-TIFS-Backdoor-Contrastive-Learning,2023-TIFS-Backdoor-Image-Encryption,2024-TIFS-Backdoor-Minimalism-is-King,2023-TIFS-person-re-identification-backdoor}.
%Targeted poisoning attacks can be formulated as a multi-task problem, where adversaries aim to cause the victim model to behave abnormally on designated samples while maintaining its proper functionality on other clean samples.
%Most existing targeted attacks focus on injecting poisoned data into the training dataset of the victim model.
%The closest work to our attack is targeted poisoning attacks against active learning~\cite{2021-Usenix-active-learning-backdoor}.
%However, their proposed attack methods require adding triggers to the attack target to induce misclassification.
%There has been limited exploration of targeted poisoning attacks without the use of triggers.
%This approach not only reduces the cost of constructing poisoned samples but also enhances the stealthiness of the attack.
%Because the attack targets lack triggers, distinguishing them from clean samples becomes much more difficult.
%Furthermore, they assume that the data in the testing phase is static and does not change over time.
%Therefore, the persistence of the attack's impact has not been thoroughly analyzed.
%Unlike untargeted poisoning attacks, targeted poisoning attacks focus on inducing specific incorrect predictions~\cite{2018-NIPS-Poison-frogs,2018-Usenix-generalized-transferability,2013-Usenix-Pollution-attacks,2024-TIFS-Backdoor-Contrastive-Learning,2023-TIFS-Backdoor-Image-Encryption,2024-TIFS-Backdoor-Minimalism-is-King,2023-TIFS-person-re-identification-backdoor}. These attacks are formulated as a multi-task problem, aiming to misclassify designated samples while preserving model functionality on clean samples.
%Most targeted attacks inject poisoned data into the training dataset. The closest related work~\cite{2021-Usenix-active-learning-backdoor} focuses on targeted poisoning in active learning but relies on triggers to induce misclassification. Limited research exists on triggerless targeted attacks, which reduce the cost of constructing poisoned samples and enhance stealthiness, making detection more difficult.
%Additionally, previous works assume static testing data, neglecting the evolving nature of data and leaving the persistence of attack impacts largely unexplored.
%\textbf{Attack Negotiation among Multiple Attackers.}
% 2024-12-11-之前的版本
%\noindent This work is broadly related to works on the survival time of Android malware and attack for Android malware detectors.
%\label{Sec: Poisoning Attacks on Android Malware Detection}
%\noindent Attacks that alter the training dataset of target model are often referred to aspoisoning attacks~\cite{2021-TDSC-Malware-F1-Measure,2021-Usenix-Poisoning-Attack-Explanation-guided-Backdoor}.
%Such attacks can be classifiedinto the following categories:
%
%\textbf{Targeted Poisoning:}
%In this type of attack, the attacker aims to manipulate the victim model's predictions towards a specific target class for certain samples.
%Some literature \cite{2021-Usenix-Poisoning-the-Semi-Supervised-learning} subdivides this attack type into two subcategories.v
%The first subcategory involves attacks that cause the target model to misclassify only a fixed set of samples.
%Attacks in the second subcategory cause the target model to misclassify all samples with a specific trigger\cite{2021-TDSC-Malware-F1-Measure,2021-Usenix-Poisoning-Attack-Explanation-guided-Backdoor,2023-LookinOut-My-Backdoor-Investigating-Backdooring-Attacks-Against-DL-driven-Malware-Detectors,2023-SP-Jigsaw-puzzle}.
%
%\textbf{Untargeted Poisoning:}
%The goal of untargeted poisoning attacks is to decline the overall performance of the victim model on all input samples.
%The challenge in untargeted attacks is that their objective is to negatively impact the performance of the target model on all data, including non-poisoned samples. 
%Therefore, the manipulated samples here need to counteract the benign data.
%
%In summary, existing research focus on the poisoning attacks of the training process of the target model.
%However, we found that poisoning attacks are also very likely to occur during the sample selection stage of the concept drift adaptation.
%Although attackers cannot directly manipulate the training dataset at this stage, they may achieve an indirect attack by influencing the training dataset updates.
%Besides, existing research lacks an exploration of whether a conversion relationship may exist between targeted and untargeted poisoning attacks.
%In the poisoning attack proposed in this study, the attacker can rapidly switch between targeted and untargeted poisoning attacks by adjusting the attack parameters.
%
%\label{Sec: Strong attacker capability settings}
%
%\definecolor{customblue}{HTML}{98ADDF}
%% 定义空心圆
%\newcommand*\emptycirc[1][1ex]{\tikz\draw (0,0) circle (#1);} 
%% 定义半圆
%\newcommand*\halfcirc[1][1ex]{%
%	\begin{tikzpicture}
%		% 先填充半圆，并且保证它与外圆对齐
%		\fill[customblue] (90:#1) arc (90:270:#1) -- cycle;
%		% 绘制完整的外部圆形
%		\draw (0,0) circle (#1);
%\end{tikzpicture}}
%% 定义实心圆（带有黑色边框）
%\newcommand*\fullcirc[1][1ex]{%
%	\begin{tikzpicture}
%		% 先填充圆形为指定颜色
%		\fill[customblue] (0,0) circle (#1);
%		% 然后在圆的外边再画一个圆的边框
%		\draw (0,0) circle (#1);
%\end{tikzpicture}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%2025-Usenix-投稿被拒版本%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\textbf{Malware Survival Time.} 
%Research on the survival time of malware primarily focuses on the dynamic changes of malware detection results and their influencing factors. 
%Shuofei Zhu et al.~\cite{2020-Usenix-Measuring-and-modeling-the-label-dynamics-of-online} are the first to observe fluctuations in the survival time of malware based on large-scale data collection spanning a year. 
%However, this research primarily focuses on the detection quality of malware detection engines and how to interpret the detection results of different engines. 
%This work attributes the fluctuations in the survival time of malware to the detection quality of the detection engines. 
%Still, there is a lack of research on the reasons behind the differences in detection quality among these engines. 
%Inspired by the large-scale data analysis and statistics~\cite{2020-Usenix-Measuring-and-modeling-the-label-dynamics-of-online}, our research focuses on the influencing factors of the survival time of new malware.
%
%\textbf{Attack for Android Malware Detectors.} 
%The attack methods targeting Android malware detectors are primarily categorized into evasion attack and poisoning attack.
%1) Evasion Attack: Attackers can obtain false-negative detection results for new malware by performing operations such as repackaging and manipulating control flow graphs~\cite{2023-TDSC-Evasion-attacks-guided-by-local-explanations-against-Android-malware-classification,2019-TIFS-Evasion-Attack-Repacking-for-ML-AMD,2019-Adversarial-example-attacks-toward-android-malware-detection-system}. 
%The currently mainstream method, proposed by Ping He et al.~\cite{2023-CCS-Query-Based-Evasion-Attack}, can achieve effective attacks under a zero-knowledge setting. 
%2) Poisoning Attack: Apart from modifying malware, attackers have also proposed poisoning attacks during the training phase of malware detection models~\cite{2021-Usenix-Poisoning-Attack-Explanation-guided-Backdoor,2021-TDSC-Malware-F1-Measure,2023-LookinOut-My-Backdoor-Investigating-Backdooring-Attacks-Against-DL-driven-Malware-Detectors}. 
%Poisoning attacks can be divided into non-targeted poisoning attacks and targeted poisoning attacks (backdoor attacks). 
%Currently, backdoor attacks are the primary form of poisoning attacks against Android malware detectors.
%The current mainstream method is to carry out backdoor attacks by adding triggers to new malware~\cite{2023-SP-backdoor-attack}, thereby prolonging its survival time. 
%However, both of the aforementioned methods require modifications to the new targeted malware, which increases the attack cost. 
%Therefore, we propose \pandora, a targeted poisoning attack method that does not require any modifications to the malware samples. 
%It effectively prolongs the survival time of new malware while significantly reducing attack costs.