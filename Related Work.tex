\section{Related Work}
\label{sec: Related work}
\textbf{Poisoning Attacks Against Active Learning}.
Zhao et al.~\cite{zhao2012sampling} investigated the security of sampling strategies in active learning. 
However, their approach assumes that attackers can remove samples from the testing data, preventing the victim model from collecting them. 
In contrast, attackers typically do not have control over the victim model’s data collection process.
Miller et al.~\cite{miller2014adversarial} discussed adversarial threats in active learning, highlighting the risk of manipulation in the sample selection process. However, their attack is indiscriminate and lacks stealth.
Vicarte et al.~\cite{2021-Usenix-active-learning-backdoor} proposed a backdoor attack against active learning, leading to misclassification of specific attack targets.
Nevertheless, their method requires embedding triggers into the attack targets, which introduces considerable overhead during the concept drift process.
This overhead arises from the need to continuously update the triggers to keep pace with the evolving victim model throughout the adaptation.
In contrast, our \pandora requires no modifications to the attack targets.

\textbf{Adversarial Concept Drift}.
Korycki et al.~\cite{2023-CCF-B-Adversarial-concept-drift-detection-under-poisoning-attacks} pointed out that existing drift detectors cannot distinguish between real and adversarial concept drift. 
Their proposed adversarial concept drift scenario assumes a strong adversary capable of directly manipulating the victim model’s training dataset, which contrasts with our setting, where only selected samples are labeled and used for training in CDA-AL.
Apruzzese et al.~\cite{apruzzese2024adversarial} analyzed the impact of adversarial perturbations occurring simultaneously with real concept drift in the context of intrusion detection.
However, their study primarily focuses on the intrusion detection scenario, with limited analysis and validation in other domains.
Moreover, their attack methods are limited to the problem space and do not account for the influence of feature space perturbations.
In addition, they typically assume that the victim model does not utilize data filtering mechanisms such as active learning.