We thank the reviewers for the constructive feedback.我们结合给出的评审意见进行了补充说明。
说明中涉及的实验数据除特殊说明外，其模型设置、数据集设置以及攻击目标选择等与正文实验设置保持一致。

### Review#A-威胁模型的描述给出更多参考文献
在现有威胁模型描述（章节3）的基础上我们进一步补充说明，并与PANDORA最相似的清洁标签投毒攻击威胁模型进行对比。
PANDORA与已有清洁标签投毒攻击都遵循投毒样本标签保持正确的基本原则。
但已有清洁标签攻击假设投毒样本标签不被恶意篡改即可进入训练集，而在CDA-AL场景下，投毒样本还必须具有高不确定性才能被主动学习机制选中并进入训练数据集。
并且经典清洁标签投毒攻击假设攻击者可以获取模型参数等信息[1]，而PANDORA在无法获取此类信息下依旧保持高攻击成功率。
此外，我们的威胁模型中认为攻击者可以获取样本特征提取方法、不确定性计算策略等信息，主要是为了验证受害者模型无法对关键技术细节保密的情况下CDA-AL方法的安全性。
该想法与目前最新研究中强调Security by Obscurity理念存在局限性，进而强调需要增强AI透明程度的情况下验证模型安全性的想法一致[2]。并且我们在rebuttal的后续实验部分补充了在约束上述假设的情况进行的测试，PANDORA攻击依旧有效。因此，PANDORA攻击在更严格的威胁模型下实现了对CDA-AL的有效攻击，揭示了其安全风险。
[1] 2018-NIPS-Poison Frogs! Targeted Clean-Label Poisoning Attacks on Neural Networks
[2] 2025-AAAI-The Pitfalls of “Security by Obscurity” and What They Mean for Transparent AI

### Review#A-攻击形式化描述
Symbols that have already appeared retain exactly the same meaning as in the main text.  
The optimization objective of the PANDORA attacker is defined as follows:  
\[
\max_{ \{ D_{poi}^{n} \}_{n=1}^{T} } \sum_{n=1}^{T} \mathbb{I}\left[ f_{\bm{\theta}_{n}}(x_{tar}) \neq y_{tar} \right]
\]
where:  
- \(x_{tar}\): the target sample under attack, with ground-truth label \(y_{tar}\);  
- \(T\): the total number of concept drift periods;  
- \(\mathbb{I}[\cdot]\): the indicator function, returning 1 if the condition holds and 0 otherwise;  
- \(D_{poi}^{n}\): the poisoning sample set injected into the test stream at the \(n\)-th round, which consists of poisoning samples generated from problem-space perturbations \(D_{\alpha}^{n}\) and from feature-space perturbations \(D_{shap}^{n}\);  
- \(f_{\bm{\theta}_{n}}\): the victim model updated on training data augmented with the poisoning set \(D_{poi}^{n}\).  

因此攻击者的目标就是通过在概念漂移过程中的每一个概念漂移周期n周期提交投毒样本$D_{poi}^{n}$影响CDA-AL的模型更新过程得到被毒害的模型$f_{\bm{\theta}}$,并确保攻击目标$x_{tar}$的预测标签不等于其真实标签，实现误分类。

### Review#A-威胁模型更加放宽为不了解不确定性策略
当攻击者无法通过查询接口获取受害者模型的不确定性计算结果且无法了解受害者模型的不确定性度量策略时，攻击者可以根据模型输出的置信度(confidence)用近似不确定性评估算法进行PANDORA的迁移攻击。为了说明该迁移的有效性，我们在APIGraph数据集上用最经典的UNCER策略作为攻击者唯一拥有的不确定性计算方法分别对HCL、CADE做PANDORA的迁移攻击。
实验结果表明PANDORA攻击依旧有效，且攻击成功率平均超过85%，其背后的原因在于同一个样本不同不确定性策略下计算结果不同，但是其相对不确定性排名相似，因此攻击者可以通过近似不确定性计算获取有效投毒种子样本，进而可以实现PANDORA攻击的有效迁移。

### Review#A-问题空间扰动深入讨论
表1给出问题空间扰动的示例操作，是为了说明攻击者能以较低的成本快速生成投毒样本，同时避免投毒样本与种子样本对于主动学习的人工分析人员过于相似，提高攻击隐蔽性。但是问题空间扰动的策略并不唯一，只要不影响样本特征的问题空间操作均可。在恶意软件等安全领域往往有成熟的工具框架可以辅助实现问题空间扰动，例如恶意软件混淆工具。在图像等非安全领域部分攻击者也可以使用添加噪声等更加隐蔽的方式实现问题空间扰动。
为了验证不同问题空间扰动策略对攻击影响的相似性，我们以恶意软件数据集AndroZoo为例补充测试了不同问题空间扰动方法下的投毒样本的特征变化情况，实验结果表明6种（内存保护、字符加密、函数虚拟化等）不同的问题空间扰动方法都保持了相同的投毒样本特征（Drebin特征提取方法），并且我们发现不同的问题空间扰动即便相互叠加也同样可以保持样本特征，意味着攻击者可以根据自己的计算资源选择简单或者是复杂的问题空间扰动策略，以及是否叠加更多问题空间扰动策略增加攻击隐蔽性。

### Review#A-概念漂移方向误导
问题空间扰动的局限性在于当无法搜索到有效的投毒攻击种子样本时会导致攻击流程中断，即测试数据中攻击目标不确定性最高。因此提出基于特征空间扰动的投毒样本生成方法（基于Shapley特征归因构造高不确定性投毒样本）。两种方法的本质区别在于产生的投毒样本的特征是否已经存在于已有真实测试样本集合中。由于特征空间扰动方法生成的样本包含原本不存在于测试数据集样本集合中的特征，因此模型学习后会误导模型对真实概念漂移方向的判断，因此称之为概念漂移方向误导，本质上是一种针对问题空间扰动局限性进行补充的特征空间扰动策略。

### Review#A-SHAP方法选型
Ledda等人的方法在CDA-AL威胁模型下不能适应最大化样本不确定性，因为该算法需要依赖模型梯度信息，而我们的威胁模型建立在攻击者不了解模型内部信息的假设下，因此我们提出了基于SHAP的改进不确定性最大化方法，攻击者只需要获取模型的预测不确定性信输出值即可，可以适应更加现实的威胁模型。

### Review#A-放宽威胁模型，不了解标记成本
当攻击者不了解受害者模型标签预算时，攻击者结合本地的计算资源采取最大化投毒样本生成策略，如图7所示的Unaware设置可以实现接近90%的攻击成功率。并且由于标记成本极为昂贵，单个投毒样本的生成成本远远小于单个样本的标记成本，因此攻击者生成投毒样本数量多于标签预算设置是更加符合实际的。

### Review#D-不确定性测量范围
首先，受害者模型主动学习过程中收集原始样本，之后提取样本特征进行不确定性计算。因此攻击者在问题空间对原始样本本身进行修改（如添加无用代码段）但不影响样本特征的情况下生成的投毒样本依旧可以保持与投毒攻击种子相似的不确定性。问题空间修改的主要目的是避免人工分析者获取原始样本后发现投毒样本之间完全相似，降低攻击的隐蔽性。

### Review#D-特征空间 vs 问题空间
特征空间攻击确实需要建立在攻击者了解受害者模型特征构造方式的基础上进行，在我们的基础威胁模型中我们认为攻击者可以获取该信息，该假设也被最新的清洁标签投毒攻击威胁模型所采用[1]。并且CDA-AL的安全性不应该建立在需要依赖保密特征构造方法的前提下，最近的研究强调了AI系统应该公开其工作原理以增强其安全性 [2]。此外，即便威胁模型进一步调整为攻击者无法获取受害者模型的特征构造方式，攻击者依旧可以收集主流的特征构造方式形成特征提取策略集合，之后通过在不同的特征构造设置下均生成投毒样本并查询投毒样本的不确定性变化进行受害者模型的特征构造方式猜测。为了探索攻击者难以获取特征情况下的攻击有效性，我们借助图像领域丰富的特征构造方法，以CIFAR-100为例测试了多种不同的特征提取方式下攻击有效性的影响，并尝试进行猜测分析，发现攻击者可以通过不确定性归因矩阵的一致性推测受害者模型的不确定性计算类别甚至具体策略。
[1] Clean-Label Graph Backdoor Attack in the Node Classification Task
[2] 2025-AAAI-The Pitfalls of “Security by Obscurity” and What They Mean for Transparent AI

### Review#C-数据集使用说明
本文研究涉及了5个不同数据集，包含真实漂移与合成漂移，并覆盖恶意软件检测、文本分析、图像识别等多种应用场景。但是考虑到不同数据集的概念漂移属性不同，因此在实验评估环节具有不同的作用。
APIGraph数据集具有较长的概念漂移时间跨度，且为真实概念漂移数据集，因此作为主要实验对象，完成攻击方法（表4、5、7以及图2、3、4、5）与防御方法有效性（图9）的验证并对攻击影响因素（图6、7）进行分析。
其中图3用于说明基于特征空间扰动的构造方法可以实现将问题空间扰动攻击失败的案例进一步实现有效攻击，y轴表示攻击目标对应的测试周期，由于攻击目标为随机抽取的138个无攻击状态下存活时间小于6个月的攻击目标，其中不包含测试周期为5的攻击目标。
其他合成概念漂移数据集（BODMAS、MNIST、SPAM）的概念漂移时间跨度较短，主要用于验证多目标攻击（表7）与防御方法的有效性（表9）。
此外，为了分析到不同概念漂移强度数据对攻击的影响，我们补充了与APIGraph数据集场景相似，但是概念漂移强度不同的AndroZoo数据集进行攻击差异分析（5.3.3所述）。

### Review#A-攻击效果解释上下文补充
PANDORA选择了CDA-AL下的概念漂移样本作为攻击目标，每一个攻击目标在CDA-AL设置下的无攻击状态都有自己原本的误分类持续时间。由于每一个概念漂移周期模型更新一次，因此误分类持续时间为概念漂移周期的整数倍。
PANDORA通过构造投毒样本可以将超过80%的攻击目标原本的误分类持续时间进行延长，并且该效果具有普遍性，我们在真实以及合成概念漂移数据集都进行了测试。

### Review#B-AndroZoo数据集补充测试


### Review#C&D-能否获取完整测试数据分布对攻击的影响
我们补充了攻击者无法获取完整测试数据分布情况下的攻击有效性分析，攻击者只能获取全局的30%、50%、70%测试数据样本。在这种情况下攻击者依旧可以实现平均75.6%的攻击成功率（APIGRaph数据集-单目标攻击）。其根本原因在于投毒样本的构造依赖投毒攻击种子，即高不确定性的良性样本，此类样本的获取并不依赖于对测试数据分布的了解，即使收集一小部分数据可能获得有效投毒攻击种子样本。该结果充分说明了攻击者在CDA-AL进行投毒攻击的优势。

### Review#A-why Fobus Target have low ASR
我们对比了Fobus家族中攻击成功与失败的情况，发现攻击失败的情况下模型学习到的与攻击目标相似（余弦相似度>0.9）的样本是攻击成功情况下的2.5倍，而这些相似样本主要来源于其他非攻击目标恶意软件。由此可见，同为Fobus家族内部的不同攻击目标存在较大的概念漂移强度（与其他恶意软件的相似性）差异，概念漂移强度更高的样本有更高的攻击成功率，该现象也符合PANDORA攻击主要针对概念漂移场景的假设。

### Review#B-实验数据的解释F1没有下降
评审意见中描述图7为多目标攻击结果且F1下降查结果10%，但是图5为多目标攻击结果，且图7不存在F1下降10%。因此我们结合图5进行说明，图5中的多目标攻击测试y轴为FNR，绿色为实施PANDORA攻击之后的攻击目标集合FNR，红色为攻击之前的FNR，F1数据没有体现在图中，但是我们的实验结果表明F1依旧保持稳定，平均0.88，且与非攻击状态差距小于0.02。

### Review#D-CDA-AL变体的考量
我们为PANDORA增加了更广泛的测试，除了现有的不确定性量化方法外增加了主动学习其他样本选择策略分支以覆盖主流的全部查询策略种类[1]，包括不确定性与多样性混合查询策略、深度贝叶斯查询策略以及密度方法查询策略。
实验结果表明PANDORA攻击在新增的三种不同查询策略上均可以实现有效攻击，攻击成功率为xx%，同时保持了攻击隐蔽性，F1达到xx%。
[1]A Survey of Deep Active Learning

### Review#A-说明对未来数据与已有知识的影响
我们分析了PANDORA攻击过程中受害者模型在测试数据的F1变化以及每一次受害者模型在投毒数据下进行模型更新后对已有训练数据的影响。我们发现PANDORA既不会影响对未来数据的学习（测试数据保持高F1，如图4、、6、7所示）也不会使得模型遗忘已有知识（PANDORA单目标以及多目标攻击下已有训练数据集上的F1依旧平均保持0.95）。主要原因在于PANDORA主要消除受害者模型对攻击目标相关数据的学习，但是不影响其他数据，并且现有CDA-AL方法主要采用微调的方法以适应数据分布的变化，因此投毒数据的影响主要体现在攻击目标无法被学习，但是不影响已有的知识和对非攻击目标的学习。

### Review#A-投毒种子的标签说明
由于不同恶意软件之间存在潜在的相互关联（如相似的漏洞利用方法），为了避免在投毒过程中使用恶意软件导致模型学习到与攻击目标相关的知识，因此投毒样本均为良性样本。

### Review#D-更加广泛的模型弱化后的攻击有效性评估
我们在原有基础上对模型弱化后的攻击有效性进行了更加广泛的评估，我们发现在xxx、xxx、xxx等方面与强攻击者能力下的差异较小，说明PANDORA即便是在更加有限的设置下同样可以对CDA-AL造成显著的威胁。

### Review#C-相关工作分析与贡献补充说明
[1] Demetrio, Luca, et al. "Functionality-preserving black-box optimization of adversarial windows malware." IEEE Transactions on Information Forensics and Security 16 (2021): 3469-3478.
[2] Grosse, Kathrin, et al. "Adversarial examples for malware detection." European symposium on research in computer security. Cham: Springer International Publishing, 2017.
上述两个工作主要聚焦于通过修改攻击目标本身构造对抗样本实现误分类，与我们的威胁模型设置不同，我们不修改攻击目标本身，而是通过投毒攻击误导模型实现对攻击目标的误分类，有助于攻击目标保持完整性进而提升隐蔽性。并且上述方法没有考虑模型更新对于攻击效果的影响，对攻击效果可持续性缺少关注。
[3] Kloft, Marius, and Pavel Laskov. "Online anomaly detection under adversarial impact." Proceedings of the thirteenth international conference on artificial intelligence and statistics. JMLR Workshop and Conference Proceedings, 2010.
上述工作与我们的攻击目标类似，也通过影响模型更新实现攻击目标的误分类，但是该方法假设攻击者可以任意向训练数据集添加一部分攻击样本，但实际中受害者模型的训练数据集有严格的筛选机制（尤其是敏感的安全场景），而我们的威胁模型更加严格，攻击者构造的投毒样本必须满足高不确定性才可进入训练数据集。此外，该方法依旧没有进行攻击有效性的持续分析。

虽然现有的部分研究关注了如何规避恶意软件检测器以及在线学习的安全风险，但是PANDORA与上述研究的威胁模型（更加严格的投毒样本限制）以及攻击目标（无需修改攻击目标本身）存在显著差异，并且PANDORA首次验证了模型持续更新下的攻击效果持久性，对于敏感领域（如恶意软件检测长期存活）具有重要的意义。

### Review#D-code repository
代码链接已更新